{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2b72e1",
   "metadata": {},
   "source": [
    "<h1>Data Ingestion</h1>\n",
    "<p> I will be using the opus books dataset. More specifically the english to french portion of the dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3a013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Transformers-Coding-and-Multi-GPU-Training-from-Scratch\\trans-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "### Using the Opus Books Dataset from Huggingface\n",
    "def data_ingestion():\n",
    "    ds = load_dataset(path=\"Helsinki-NLP/opus_books\", name=\"en-fr\")\n",
    "    train_test_data=ds['train'].train_test_split(test_size=0.2,seed=42)\n",
    "    test_data=train_test_data['test']\n",
    "    train_val_split=train_test_data['train'].train_test_split(test_size=0.2,seed=42)\n",
    "    train_data=train_val_split['train']\n",
    "    validation_data=train_val_split['test']\n",
    "    return train_data,validation_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c724258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,validation_data,test_data=data_ingestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f1262",
   "metadata": {},
   "source": [
    "<h5> languagewise sentence generator function </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d218ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "    for pair in ds:\n",
    "        # print(pair)\n",
    "        yield pair['translation'][lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4596555",
   "metadata": {},
   "source": [
    "### Build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79797b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "def build_tokenizer(config,ds,lang):\n",
    "    tokenizer_path=Path(config['tokenizer_file'].format(lang))\n",
    "    \n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        trainer=WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],min_frequency=1)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb67d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The tokenizer will be trained on the train set of the data only.\n",
    "##There will be 2 separate tokenizer, one will be for english and other will be for french.\n",
    "tokenizer_en=build_tokenizer({'tokenizer_file':'tokenizer_en.json'},train_data,'en')\n",
    "tokenizer_fr=build_tokenizer({'tokenizer_file':'tokenizer_fr.json'},train_data,'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a519660",
   "metadata": {},
   "source": [
    "<p> max seq len will be needed during positional embedding layer creation. Incase a input comes bigger than mex seq len during inference, we have to truncate the sequence to max seq len. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7963ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to get the max seq len that is possible in the dataset.\n",
    "def get_max_seq_len(train_data,test_data,validation_data):\n",
    "    max_len=0\n",
    "    for data in train_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "\n",
    "    for data in test_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "\n",
    "    for data in validation_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "    return max_len    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bfc23",
   "metadata": {},
   "source": [
    "### Causal Mask Generator Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19314951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def casual_mask_generator(size):\n",
    "    mask=torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int)\n",
    "    return mask==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59508088",
   "metadata": {},
   "source": [
    "### Creating the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b7e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class opusDataset_En_to_Fr(Dataset):\n",
    "    def __init__(self,data,tokenizer_en,tokenizer_fr):\n",
    "        super().__init__()\n",
    "        self.raw_data=data\n",
    "        self.tokenizer_en=tokenizer_en\n",
    "        self.tokenizer_fr=tokenizer_fr\n",
    "\n",
    "        self.sos_token=torch.tensor([self.tokenizer_en.token_to_id(\"[SOS]\")],dtype=torch.int64)  ### start of sequence token\n",
    "        self.eos_token=torch.tensor([self.tokenizer_en.token_to_id(\"[EOS]\")],dtype=torch.int64)  ### End of sequence token\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_en=self.raw_data[index]['translation']['en']\n",
    "        data_fr=self.raw_data[index]['translation']['fr']\n",
    "        encoded_data_en=torch.tensor(self.tokenizer_en.encode(data_en).ids,dtype=torch.int64)\n",
    "        encoded_data_fr=torch.tensor(self.tokenizer_fr.encode(data_fr).ids,dtype=torch.int64)\n",
    "\n",
    "        final_encoded_en=torch.cat([\n",
    "            encoded_data_en,\n",
    "            self.eos_token,\n",
    "        ]\n",
    "        )\n",
    "        final_encoded_fr=torch.cat([\n",
    "            self.sos_token,\n",
    "            encoded_data_fr,\n",
    "        ])\n",
    "\n",
    "        target_encoded_fr=torch.cat([\n",
    "            encoded_data_fr,\n",
    "            self.eos_token,\n",
    "        ])\n",
    "\n",
    "        return {\n",
    "            'encoder_input':final_encoded_en,\n",
    "            'decoder_input':final_encoded_fr,\n",
    "            'target_output':target_encoded_fr,\n",
    "            'src_sentence':data_en,\n",
    "            'tgt_sentence':data_fr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a9411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=opusDataset_En_to_Fr(train_data,tokenizer_en,tokenizer_fr)\n",
    "test_dataset=opusDataset_En_to_Fr(test_data,tokenizer_en,tokenizer_fr)\n",
    "validation_dataset=opusDataset_En_to_Fr(validation_data,tokenizer_en,tokenizer_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae985a61",
   "metadata": {},
   "source": [
    "### Creating the Dataloader class\n",
    "<p> At first I will be creating a custom collate function for the loader, to pad the sequence of a batch to even length. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(input_list):\n",
    "\n",
    "    max_length_in_en_batch=0\n",
    "    max_length_in_fr_batch=0\n",
    "    max_length_in_tfr_batch=0\n",
    "\n",
    "    for data in input_list:\n",
    "        max_length_in_en_batch=max(max_length_in_en_batch,len(data['encoder_input']))\n",
    "\n",
    "    for data in input_list:\n",
    "        max_length_in_fr_batch=max(max_length_in_fr_batch,len(data['decoder_input']))\n",
    "\n",
    "    for data in input_list:\n",
    "        max_length_in_tfr_batch=max(max_length_in_tfr_batch,len(data['target_output']))\n",
    "\n",
    "    encoder_inputs=[]\n",
    "    decoder_inputs=[]\n",
    "    target_outputs=[]\n",
    "    encoder_masks=[]\n",
    "    decoder_masks=[]\n",
    "    src_sentences=[]\n",
    "    tgt_sentences=[]\n",
    "\n",
    "    pad_en=torch.tensor([tokenizer_en.token_to_id('PAD')])\n",
    "    pad_fr=torch.tensor([tokenizer_fr.token_to_id('PAD')])\n",
    "    \n",
    "\n",
    "    for data in input_list:\n",
    "        \n",
    "        encoder_input= torch.cat(\n",
    "            [\n",
    "                data['encoder_input'],\n",
    "                torch.tensor(pad_en*(max_length_in_en_batch-len(data['encoder_input'])))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        decoder_input= torch.cat(\n",
    "            [\n",
    "                data['decoder_input'],\n",
    "                torch.tensor(pad_fr*(max_length_in_fr_batch-len(data['decoder_input'])))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        target_output = torch.cat(\n",
    "            [\n",
    "                data['target_output'],\n",
    "                torch.tensor(pad_fr*(max_length_in_tfr_batch-len(data['target_output'])))\n",
    "            ]\n",
    "        )\n",
    "        encoder_mask=(encoder_input!=torch.tensor(tokenizer_en.token_to_id('[PAD]'))).unsqueeze(0).unsqueeze(0).int()\n",
    "        decoder_mask=((decoder_input!=torch.tensor(tokenizer_fr.token_to_id('[PAD]'))).unsqueeze(0).unsqueeze(0).int()) & casual_mask_generator(len(decoder_input))\n",
    "\n",
    "\n",
    "\n",
    "        encoder_inputs.append(encoder_input)\n",
    "        decoder_inputs.append(decoder_input)\n",
    "        target_outputs.append(target_output)\n",
    "        encoder_masks.append(encoder_mask)\n",
    "        decoder_masks.append(decoder_mask)\n",
    "        src_sentences.append(data['src_sentence'])\n",
    "        tgt_sentences.append(data['tgt_sentence'])\n",
    "\n",
    "    return{\n",
    "\n",
    "        'encoder_input':torch.stack(encoder_inputs),\n",
    "        'decoder_input':torch.stack(decoder_inputs),\n",
    "        'target_output':torch.stack(target_outputs),\n",
    "        'encoder_mask':torch.stack(encoder_masks),\n",
    "        'decoder_mask':torch.stack(decoder_masks),\n",
    "        'src_sentence': src_sentences,\n",
    "        'tgt_sentence': tgt_sentences\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109a757d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor([1]))*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd72549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
