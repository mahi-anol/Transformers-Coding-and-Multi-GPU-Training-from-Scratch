{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2b72e1",
   "metadata": {},
   "source": [
    "<h1>Data Ingestion</h1>\n",
    "<p> I will be using the opus books dataset. More specifically the english to french portion of the dataset. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc3a013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "### Using the Opus Books Dataset from Huggingface\n",
    "def data_ingestion():\n",
    "    ds = load_dataset(path=\"Helsinki-NLP/opus_books\", name=\"en-fr\")\n",
    "    train_test_data=ds['train'].train_test_split(test_size=0.2,seed=42)\n",
    "    test_data=train_test_data['test']\n",
    "    train_val_split=train_test_data['train'].train_test_split(test_size=0.2,seed=42)\n",
    "    train_data=train_val_split['train']\n",
    "    validation_data=train_val_split['test']\n",
    "    return train_data,validation_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c724258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,validation_data,test_data=data_ingestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f1262",
   "metadata": {},
   "source": [
    "<h5> languagewise sentence generator function </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d218ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "    for pair in ds:\n",
    "        # print(pair)\n",
    "        yield pair['translation'][lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4596555",
   "metadata": {},
   "source": [
    "### Build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d79797b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "def build_tokenizer(config,ds,lang):\n",
    "    tokenizer_path=Path(config['tokenizer_file'].format(lang))\n",
    "    \n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        trainer=WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],min_frequency=1)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb67d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The tokenizer will be trained on the train set of the data only.\n",
    "##There will be 2 separate tokenizer, one will be for english and other will be for french.\n",
    "tokenizer_en=build_tokenizer({'tokenizer_file':'tokenizer_en.json'},train_data,'en')\n",
    "tokenizer_fr=build_tokenizer({'tokenizer_file':'tokenizer_fr.json'},train_data,'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a519660",
   "metadata": {},
   "source": [
    "<p> max seq len will be needed during positional embedding layer creation. Incase a input comes bigger than mex seq len during inference, we have to truncate the sequence to max seq len. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7963ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to get the max seq len that is possible in the dataset.\n",
    "def get_max_seq_len(train_data,test_data,validation_data):\n",
    "    max_len=0\n",
    "    for data in train_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "\n",
    "    for data in test_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "\n",
    "    for data in validation_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "    return max_len    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bfc23",
   "metadata": {},
   "source": [
    "### Causal Mask Generator Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19314951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def causal_mask_generator(size):\n",
    "    mask=torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int)\n",
    "    return mask==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59508088",
   "metadata": {},
   "source": [
    "### Creating the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35b7e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class opusDataset_En_to_Fr(Dataset):\n",
    "    def __init__(self,data,tokenizer_en,tokenizer_fr):\n",
    "        super().__init__()\n",
    "        self.raw_data=data\n",
    "        self.tokenizer_en=tokenizer_en\n",
    "        self.tokenizer_fr=tokenizer_fr\n",
    "\n",
    "        self.sos_token=torch.tensor([self.tokenizer_en.token_to_id(\"[SOS]\")],dtype=torch.int64)  ### start of sequence token\n",
    "        self.eos_token=torch.tensor([self.tokenizer_en.token_to_id(\"[EOS]\")],dtype=torch.int64)  ### End of sequence token\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_en=self.raw_data[index]['translation']['en']\n",
    "        data_fr=self.raw_data[index]['translation']['fr']\n",
    "        encoded_data_en=torch.tensor(self.tokenizer_en.encode(data_en).ids,dtype=torch.int64)\n",
    "        encoded_data_fr=torch.tensor(self.tokenizer_fr.encode(data_fr).ids,dtype=torch.int64)\n",
    "\n",
    "        final_encoded_en=torch.cat([\n",
    "            encoded_data_en,\n",
    "            self.eos_token,\n",
    "        ]\n",
    "        )\n",
    "        final_encoded_fr=torch.cat([\n",
    "            self.sos_token,\n",
    "            encoded_data_fr,\n",
    "        ])\n",
    "\n",
    "        target_encoded_fr=torch.cat([\n",
    "            encoded_data_fr,\n",
    "            self.eos_token,\n",
    "        ])\n",
    "\n",
    "        return {\n",
    "            'encoder_input':final_encoded_en,\n",
    "            'decoder_input':final_encoded_fr,\n",
    "            'target_output':target_encoded_fr,\n",
    "            'src_sentence':data_en,\n",
    "            'tgt_sentence':data_fr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66a9411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=opusDataset_En_to_Fr(train_data,tokenizer_en,tokenizer_fr)\n",
    "test_dataset=opusDataset_En_to_Fr(test_data,tokenizer_en,tokenizer_fr)\n",
    "validation_dataset=opusDataset_En_to_Fr(validation_data,tokenizer_en,tokenizer_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae985a61",
   "metadata": {},
   "source": [
    "### Creating the Dataloader class\n",
    "<p> At first I will be creating a custom collate function for the loader, to pad the sequence of a batch to even length. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58db616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(input_list):\n",
    "\n",
    "    max_length_in_en_batch=0\n",
    "    max_length_in_fr_batch=0\n",
    "    max_length_in_tfr_batch=0\n",
    "\n",
    "    for data in input_list:\n",
    "        max_length_in_en_batch=max(max_length_in_en_batch,len(data['encoder_input']))\n",
    "        max_length_in_fr_batch=max(max_length_in_fr_batch,len(data['decoder_input']))\n",
    "        max_length_in_tfr_batch=max(max_length_in_tfr_batch,len(data['target_output']))\n",
    "\n",
    "    encoder_inputs=[]\n",
    "    decoder_inputs=[]\n",
    "    target_outputs=[]\n",
    "    encoder_masks=[]\n",
    "    decoder_masks=[]\n",
    "    src_sentences=[]\n",
    "    tgt_sentences=[]\n",
    "\n",
    "    pad_en=torch.tensor([tokenizer_en.token_to_id('[PAD]')])\n",
    "    pad_fr=torch.tensor([tokenizer_fr.token_to_id('[PAD]')])\n",
    "    \n",
    "\n",
    "    for data in input_list:\n",
    "        \n",
    "        encoder_input= torch.cat(\n",
    "            [\n",
    "                data['encoder_input'],\n",
    "                torch.full((max_length_in_en_batch-len(data['encoder_input']),),pad_en.item())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        decoder_input= torch.cat(\n",
    "            [\n",
    "                data['decoder_input'],\n",
    "                torch.full((max_length_in_fr_batch-len(data['decoder_input']),),pad_fr.item())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        target_output = torch.cat(\n",
    "            [\n",
    "                data['target_output'],\n",
    "                torch.full((max_length_in_fr_batch-len(data['target_output']),),pad_fr.item())\n",
    "            ]\n",
    "        )\n",
    "        encoder_mask=(encoder_input!=pad_en.item()).unsqueeze(0).unsqueeze(0).int()\n",
    "        decoder_mask=((decoder_input!=pad_fr.item()).unsqueeze(0).unsqueeze(0).int()) & causal_mask_generator(len(decoder_input))\n",
    "\n",
    "\n",
    "\n",
    "        encoder_inputs.append(encoder_input)\n",
    "        decoder_inputs.append(decoder_input)\n",
    "        target_outputs.append(target_output)\n",
    "        encoder_masks.append(encoder_mask)\n",
    "        decoder_masks.append(decoder_mask)\n",
    "        src_sentences.append(data['src_sentence'])\n",
    "        tgt_sentences.append(data['tgt_sentence'])\n",
    "\n",
    "    return{\n",
    "\n",
    "        'encoder_input':torch.stack(encoder_inputs),\n",
    "        'decoder_input':torch.stack(decoder_inputs),\n",
    "        'target_output':torch.stack(target_outputs),\n",
    "        'encoder_mask':torch.stack(encoder_masks),\n",
    "        'decoder_mask':torch.stack(decoder_masks),\n",
    "        'src_sentence': src_sentences,\n",
    "        'tgt_sentence': tgt_sentences\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "edd72549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader=DataLoader(train_dataset,batch_size=2,shuffle=True,collate_fn=custom_collate)\n",
    "test_loader=DataLoader(test_dataset,batch_size=2,shuffle=False,collate_fn=custom_collate)\n",
    "val_loader=DataLoader(validation_dataset,batch_size=2,shuffle=False,collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2689a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20])\n",
      "torch.Size([2, 23])\n",
      "torch.Size([2, 23])\n",
      "torch.Size([2, 1, 1, 20])\n",
      "torch.Size([2, 1, 23, 23])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['encoder_input'].shape)\n",
    "    print(batch['decoder_input'].shape)\n",
    "    print(batch['target_output'].shape)\n",
    "    print(batch['encoder_mask'].shape)\n",
    "    print(batch['decoder_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485453db",
   "metadata": {},
   "source": [
    "### Modeling Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8213210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f29de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.emb_dim=emb_dim\n",
    "        self.embedding_layer=nn.Embedding(self.vocab_size,self.emb_dim,dtype=torch.float32)\n",
    "    def forward(self,x):\n",
    "        embeddings=self.embedding_layer(x)\n",
    "        return embeddings*math.sqrt(self.emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bff7f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncodingLayer(nn.Module):\n",
    "    def __init__(self,max_seq_len,emb_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        static_positional_info=torch.zeros((self.max_seq_len,self.emb_dim),dtype=torch.float32)\n",
    "        positions=torch.arange(0,max_seq_len,dtype=torch.float32).reshape(max_seq_len,-1)\n",
    "        indices_for_denominator=torch.arange(0,emb_dim,2,dtype=torch.float32) ### 2i\n",
    "        denominators=torch.exp((-2*indices_for_denominator*math.log(1e4))/emb_dim)\n",
    "        static_positional_info[:,0::2]=torch.sin(positions*denominators)\n",
    "        static_positional_info[:,1::2]=torch.cos(positions*denominators)\n",
    "        self.register_buffer('static_positional_info',static_positional_info)\n",
    "    def forward(self,x):\n",
    "        position_encoded_embedding=x+self.static_positional_info[:x.shape[1],:]\n",
    "        dropped_embeddings=self.dropout(position_encoded_embedding)\n",
    "        return dropped_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "105f5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,dropout):\n",
    "        super().__init__()\n",
    "        assert emb_dim%n_heads==0 ### checking if multi head splitting is possible.\n",
    "        self.w_q=nn.Linear(emb_dim,emb_dim,dtype=torch.float32)\n",
    "        self.w_k=nn.Linear(emb_dim,emb_dim,dtype=torch.float32)\n",
    "        self.w_v=nn.Linear(emb_dim,emb_dim,dtype=torch.float32)\n",
    "        self.w_o=nn.Linear(emb_dim,emb_dim,dtype=torch.float32) ### multi-head-projection-layer\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.single_head_dim=self.emb_dim//n_heads\n",
    "        self.n_heads=n_heads\n",
    "\n",
    "    @staticmethod\n",
    "    def contextual_embedding(m_q,m_k,m_v,per_head_emb_dim,mask=None,dropout=None):\n",
    "        ### return contexual embedding and attention scores\n",
    "        attention_scores=(m_q@m_k.transpose(-1,-2))/math.sqrt(per_head_emb_dim)\n",
    "        ##batch,head,seq_f,dim @ batch,head,dim,seqe==batch,head,seq_f,seq_e\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask==0,value=float('-inf'))\n",
    "        normalized_attention_scores=torch.softmax(attention_scores,dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            dropped_normalized_attention_scores=dropout(normalized_attention_scores)\n",
    "        ### batch,head,seq_f,seq_e @ batch,head,seqe,dim=batch,head,seqf,dim\n",
    "        contexual_embeddings=dropped_normalized_attention_scores@m_v\n",
    "        return dropped_normalized_attention_scores,contexual_embeddings\n",
    "    \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        query=self.w_q(q) ### batch, seqeunce, dim\n",
    "        key=self.w_k(k)  ### batch, seq,dim\n",
    "        value=self.w_v(v) ##batch,seq, dim\n",
    "\n",
    "        multihead_query=query.view(query.shape[0],query.shape[1],self.n_heads,self.single_head_dim).transpose(-3,-2) ##batch,seq,head,dim -> batch,head,seq,dim\n",
    "        multihead_key=key.view(key.shape[0],key.shape[1],self.n_heads,self.single_head_dim).transpose(-3,-2) ##batch,seq,head,dim -> batch,head,seq,dim\n",
    "        multihead_value=value.view(value.shape[0],value.shape[1],self.n_heads,self.single_head_dim).transpose(-3,-2) ## batch, seq,head,dim\n",
    "        _,contextual_embeddings=multiHeadAttentionBlock.contextual_embedding(multihead_query,multihead_key,multihead_value,self.single_head_dim,mask,self.dropout) # return batch,head,seq,dim\n",
    "        final_contextual_embeddings=contextual_embeddings.transpose(-3,-2).contiguous().view(query.shape[0],query.shape[1],self.n_heads*self.single_head_dim) ### \n",
    "        multihead_final_contextual_embedding_proj=self.w_o(final_contextual_embeddings)\n",
    "        dropped_multihead_final_contextual_embedding_proj=self.dropout(multihead_final_contextual_embedding_proj)\n",
    "        return dropped_multihead_final_contextual_embedding_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70b8d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerNormalizationBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim,dtype=torch.float32))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim,dtype=torch.float32))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        standard_deviation=x.std(dim=-1,keepdim=True,unbiased=False)\n",
    "        normalized_x=(x-mean)/(standard_deviation+self.eps)\n",
    "        scale_n_shift=self.scale*normalized_x+self.shift\n",
    "        return scale_n_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40cdcef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipConnection(nn.Module):\n",
    "    def __init__(self,dropout):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        output=x+sublayer(x)\n",
    "        dropped_output=self.dropout(output)\n",
    "        return dropped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6bc96d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward_block(nn.Module):\n",
    "    ### Expansion Contraction layer.....\n",
    "    def __init__(self,emb_dim,expand_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.expand_dim=expand_dim\n",
    "        self.dropout=dropout\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(emb_dim,expand_dim,dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(expand_dim,emb_dim,dtype=torch.float32),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        output=self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91398157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.ff_dropout=ff_dropout\n",
    "        self.expand_dim=expand_dim\n",
    "        self.mha_dropout=mha_dropout\n",
    "        self.sk_dropout=sk_dropout\n",
    "        self.mha_block=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout)\n",
    "        self.feed_forward_block=feed_forward_block(self.emb_dim,self.expand_dim,self.ff_dropout)\n",
    "        self.skip_connections=nn.ModuleList([skipConnection(self.sk_dropout) for _ in range(2)])\n",
    "        self.layerNormalizationBlocks=nn.ModuleList([layerNormalizationBlock(self.emb_dim) for _ in range(2)])\n",
    "    def forward(self,x,encoder_mask=None):\n",
    "        output1=self.skip_connections[0](x,lambda x: self.mha_block(x,x,x,encoder_mask))\n",
    "        layer_normalized_output1=self.layerNormalizationBlocks[0](output1)\n",
    "        output2=self.skip_connections[1](layer_normalized_output1,self.feed_forward_block)\n",
    "        layer_normalized_output2=self.layerNormalizationBlocks[1](output2)\n",
    "        return layer_normalized_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6647148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,no_of_enc_blk,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.enc_blks=nn.ModuleList([encoderBlock(emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout) for _ in range(no_of_enc_blk)])\n",
    "    def forward(self,x,encoder_mask=None):\n",
    "        for blk in self.enc_blks:\n",
    "            x=blk(x,encoder_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff620f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.ff_dropout=ff_dropout\n",
    "        self.expand_dim=expand_dim\n",
    "        self.mha_dropout=mha_dropout\n",
    "        self.sk_dropout=sk_dropout\n",
    "        self.mha_block1=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout) ### casual attention block\n",
    "        self.mha_block2=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout) #### cross attention block\n",
    "        self.feed_forward_block=feed_forward_block(self.emb_dim,self.expand_dim,self.ff_dropout)\n",
    "        self.skip_connections=nn.ModuleList([skipConnection(self.sk_dropout) for _ in range(3)])\n",
    "        self.layerNormalizationBlocks=nn.ModuleList([layerNormalizationBlock(self.emb_dim) for _ in range(3)])\n",
    "    def forward(self,x,enc_out,decoder_mask=None,encoder_mask=None):\n",
    "        output1=self.skip_connections[0](x,lambda x: self.mha_block1(x,x,x,decoder_mask)) ### causal attention\n",
    "        layer_normalized_output1=self.layerNormalizationBlocks[0](output1)\n",
    "        output2=self.skip_connections[1](layer_normalized_output1,lambda x: self.mha_block2(x,enc_out,enc_out,encoder_mask))### Query from decoder, k and v from encoder.\n",
    "        layer_normalized_output2=self.layerNormalizationBlocks[1](output2)\n",
    "        output3=self.skip_connections[2](layer_normalized_output2,self.feed_forward_block)\n",
    "        layer_normalized_output3=self.layerNormalizationBlocks[2](output3)\n",
    "        return layer_normalized_output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d0ece02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self,no_of_dec_blk,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.dec_blks=nn.ModuleList([decoderBlock(emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout) for _ in range(no_of_dec_blk)])\n",
    "    def forward(self,x,encoder_output,decoder_mask=None,encoder_mask=None):\n",
    "        for blk in self.dec_blks:\n",
    "            x=blk(x,encoder_output,decoder_mask,encoder_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6eb891f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class finalProjectionLayer(nn.Module):\n",
    "    def __init__(self,emb_dim,vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(emb_dim,vocab_size,dtype=torch.float32)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output=self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85f552a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformers(nn.Module):\n",
    "    def __init__(self,model_config,tokenizer_config):\n",
    "        super().__init__()\n",
    "        self.encoder_emb_layer=inputEmbeddingLayer(tokenizer_config['en_vocab_size'],model_config['enc_cfg']['emb_dim'])\n",
    "        self.enc_positional_emb_layer=positionalEncodingLayer(model_config['enc_max_seq_len'],model_config['enc_cfg']['emb_dim'],model_config['enc_cfg']['pos_emb_dropout'])\n",
    "        self.encoder=encoder(\n",
    "            no_of_enc_blk=model_config['enc_cfg']['no_of_enc_blk'],\n",
    "            emb_dim=model_config['enc_cfg']['emb_dim'],\n",
    "            n_heads=model_config['enc_cfg']['n_heads'],\n",
    "            mha_dropout=model_config['enc_cfg']['mha_dropout'],\n",
    "            expand_dim=model_config['enc_cfg']['expand_dim'],\n",
    "            ff_dropout=model_config['enc_cfg']['ff_dropout'],\n",
    "            sk_dropout=model_config['enc_cfg']['sk_dropout']\n",
    "        )\n",
    "        \n",
    "        self.decoder_emb_layer=inputEmbeddingLayer(tokenizer_config['fr_vocab_size'],model_config['dec_cfg']['emb_dim'])\n",
    "        self.dec_positional_emb_layer=positionalEncodingLayer(model_config['dec_max_seq_len'],model_config['dec_cfg']['emb_dim'],model_config['dec_cfg']['pos_emb_dropout'])\n",
    "        self.decoder=decoder(\n",
    "            no_of_dec_blk=model_config['dec_cfg']['no_of_dec_blk'],\n",
    "            emb_dim=model_config['dec_cfg']['emb_dim'],\n",
    "            n_heads=model_config['dec_cfg']['n_heads'],\n",
    "            mha_dropout=model_config['dec_cfg']['mha_dropout'],\n",
    "            expand_dim=model_config['dec_cfg']['expand_dim'],\n",
    "            ff_dropout=model_config['dec_cfg']['ff_dropout'],\n",
    "            sk_dropout=model_config['dec_cfg']['sk_dropout']\n",
    "        )\n",
    "        self.decoder_final_projection=finalProjectionLayer(model_config['dec_cfg']['emb_dim'],tokenizer_config['fr_vocab_size'])\n",
    "\n",
    "    def encode(self,encoder_input,encoder_mask=None):\n",
    "        encoder_input_embedding=self.encoder_emb_layer(encoder_input)\n",
    "        positional_encoded_input_embedding=self.enc_positional_emb_layer(encoder_input_embedding)\n",
    "        encoder_contexual_embedding=self.encoder(positional_encoded_input_embedding,encoder_mask)\n",
    "        return encoder_contexual_embedding\n",
    "\n",
    "    def decode(self,decoder_input,encoder_output,decoder_mask,encoder_mask):\n",
    "        decoder_input_embedding=self.decoder_emb_layer(decoder_input)\n",
    "        positional_encoded_input_embedding=self.dec_positional_emb_layer(decoder_input_embedding) ### of decoder\n",
    "        decoder_contexual_embedding=self.decoder(positional_encoded_input_embedding,encoder_output,decoder_mask,encoder_mask)\n",
    "        final_output=self.decoder_final_projection(decoder_contexual_embedding)\n",
    "        return final_output\n",
    "    \n",
    "    ###forward will be used during training.\n",
    "    def forward(self,decoder_input,encoder_input,decoder_mask,encoder_mask):\n",
    "        encoder_output=self.encode(encoder_input,encoder_mask)\n",
    "        decoder_output=self.decode(decoder_input,encoder_output,decoder_mask,encoder_mask)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c918a",
   "metadata": {},
   "source": [
    "### creating configs for building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9e846db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration dictionary\n",
    "model_config = {\n",
    "    \"enc_max_seq_len\": get_max_seq_len(train_data,validation_data,test_data)+2,   # Max source sequence length  Extra +2 for start and end token\n",
    "    \"dec_max_seq_len\": get_max_seq_len(train_data,validation_data,test_data)+2,   # Max target sequence length\n",
    "    \"enc_cfg\": {\n",
    "        \"emb_dim\": 256,\n",
    "        \"no_of_enc_blk\": 3,\n",
    "        \"n_heads\": 4,\n",
    "        \"pos_emb_dropout\":0.1,\n",
    "        \"mha_dropout\": 0.1,\n",
    "        \"expand_dim\": 1024,\n",
    "        \"ff_dropout\": 0.1,\n",
    "        \"sk_dropout\": 0.1\n",
    "    },\n",
    "    \"dec_cfg\": {\n",
    "        \"emb_dim\": 256, ### Embedding dimention\n",
    "        \"no_of_dec_blk\": 3, ## No of decoder block\n",
    "        \"pos_emb_dropout\":0.1, # dropout rate after applying  positional embedding.\n",
    "        \"n_heads\": 4, ### no of head\n",
    "        \"mha_dropout\": 0.1, ### multihead attentions dropout in attention scores\n",
    "        \"expand_dim\": 1024, ### fee forward intermediate expand dim\n",
    "        \"ff_dropout\": 0.1, ### feed forward layer dropout\n",
    "        \"sk_dropout\": 0.1 ##skip connections dropout\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tokenizer configuration dictionary\n",
    "tokenizer_config = {\n",
    "    # \"vocab_size\": 30000,  # Vocabulary size of source & target tokenizer\n",
    "    'en_vocab_size':tokenizer_en.get_vocab_size(),\n",
    "    'fr_vocab_size':tokenizer_fr.get_vocab_size()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fff2f",
   "metadata": {},
   "source": [
    "### building model and weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8eb92e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer=transformers(model_config,tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79ecb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in Transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e75f943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_config={\n",
    "#     'epochs':3,\n",
    "#     'optimizer_lr':1e-4,\n",
    "#     'optimizer_eps':1e-8,\n",
    "#     'label_smoothing':0.1,\n",
    "#     'batch_size':2,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bfa81",
   "metadata": {},
   "source": [
    "### Model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc27bb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                 Model Summary                                  \n",
      "================================================================================\n",
      "Layer Name                                    Param #    Trainable                Shape\n",
      "--------------------------------------------------------------------------------\n",
      "encoder_emb_layer.embedding_layer.weight    7,680,000         True         (30000, 256)\n",
      "encoder.enc_blks.0.mha_block.w_q.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.0.mha_block.w_q.bias             256         True               (256,)\n",
      "encoder.enc_blks.0.mha_block.w_k.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.0.mha_block.w_k.bias             256         True               (256,)\n",
      "encoder.enc_blks.0.mha_block.w_v.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.0.mha_block.w_v.bias             256         True               (256,)\n",
      "encoder.enc_blks.0.mha_block.w_o.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.0.mha_block.w_o.bias             256         True               (256,)\n",
      "encoder.enc_blks.0.feed_forward_block.network.0.weight      262,144         True          (1024, 256)\n",
      "encoder.enc_blks.0.feed_forward_block.network.0.bias        1,024         True              (1024,)\n",
      "encoder.enc_blks.0.feed_forward_block.network.3.weight      262,144         True          (256, 1024)\n",
      "encoder.enc_blks.0.feed_forward_block.network.3.bias          256         True               (256,)\n",
      "encoder.enc_blks.0.layerNormalizationBlocks.0.scale          256         True               (256,)\n",
      "encoder.enc_blks.0.layerNormalizationBlocks.0.shift          256         True               (256,)\n",
      "encoder.enc_blks.0.layerNormalizationBlocks.1.scale          256         True               (256,)\n",
      "encoder.enc_blks.0.layerNormalizationBlocks.1.shift          256         True               (256,)\n",
      "encoder.enc_blks.1.mha_block.w_q.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.1.mha_block.w_q.bias             256         True               (256,)\n",
      "encoder.enc_blks.1.mha_block.w_k.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.1.mha_block.w_k.bias             256         True               (256,)\n",
      "encoder.enc_blks.1.mha_block.w_v.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.1.mha_block.w_v.bias             256         True               (256,)\n",
      "encoder.enc_blks.1.mha_block.w_o.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.1.mha_block.w_o.bias             256         True               (256,)\n",
      "encoder.enc_blks.1.feed_forward_block.network.0.weight      262,144         True          (1024, 256)\n",
      "encoder.enc_blks.1.feed_forward_block.network.0.bias        1,024         True              (1024,)\n",
      "encoder.enc_blks.1.feed_forward_block.network.3.weight      262,144         True          (256, 1024)\n",
      "encoder.enc_blks.1.feed_forward_block.network.3.bias          256         True               (256,)\n",
      "encoder.enc_blks.1.layerNormalizationBlocks.0.scale          256         True               (256,)\n",
      "encoder.enc_blks.1.layerNormalizationBlocks.0.shift          256         True               (256,)\n",
      "encoder.enc_blks.1.layerNormalizationBlocks.1.scale          256         True               (256,)\n",
      "encoder.enc_blks.1.layerNormalizationBlocks.1.shift          256         True               (256,)\n",
      "encoder.enc_blks.2.mha_block.w_q.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.2.mha_block.w_q.bias             256         True               (256,)\n",
      "encoder.enc_blks.2.mha_block.w_k.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.2.mha_block.w_k.bias             256         True               (256,)\n",
      "encoder.enc_blks.2.mha_block.w_v.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.2.mha_block.w_v.bias             256         True               (256,)\n",
      "encoder.enc_blks.2.mha_block.w_o.weight        65,536         True           (256, 256)\n",
      "encoder.enc_blks.2.mha_block.w_o.bias             256         True               (256,)\n",
      "encoder.enc_blks.2.feed_forward_block.network.0.weight      262,144         True          (1024, 256)\n",
      "encoder.enc_blks.2.feed_forward_block.network.0.bias        1,024         True              (1024,)\n",
      "encoder.enc_blks.2.feed_forward_block.network.3.weight      262,144         True          (256, 1024)\n",
      "encoder.enc_blks.2.feed_forward_block.network.3.bias          256         True               (256,)\n",
      "encoder.enc_blks.2.layerNormalizationBlocks.0.scale          256         True               (256,)\n",
      "encoder.enc_blks.2.layerNormalizationBlocks.0.shift          256         True               (256,)\n",
      "encoder.enc_blks.2.layerNormalizationBlocks.1.scale          256         True               (256,)\n",
      "encoder.enc_blks.2.layerNormalizationBlocks.1.shift          256         True               (256,)\n",
      "decoder_emb_layer.embedding_layer.weight    7,680,000         True         (30000, 256)\n",
      "decoder.dec_blks.0.mha_block1.w_q.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block1.w_q.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.mha_block1.w_k.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block1.w_k.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.mha_block1.w_v.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block1.w_v.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.mha_block1.w_o.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block1.w_o.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.mha_block2.w_q.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block2.w_q.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.mha_block2.w_k.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block2.w_k.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.mha_block2.w_v.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block2.w_v.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.mha_block2.w_o.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.0.mha_block2.w_o.bias            256         True               (256,)\n",
      "decoder.dec_blks.0.feed_forward_block.network.0.weight      262,144         True          (1024, 256)\n",
      "decoder.dec_blks.0.feed_forward_block.network.0.bias        1,024         True              (1024,)\n",
      "decoder.dec_blks.0.feed_forward_block.network.3.weight      262,144         True          (256, 1024)\n",
      "decoder.dec_blks.0.feed_forward_block.network.3.bias          256         True               (256,)\n",
      "decoder.dec_blks.0.layerNormalizationBlocks.0.scale          256         True               (256,)\n",
      "decoder.dec_blks.0.layerNormalizationBlocks.0.shift          256         True               (256,)\n",
      "decoder.dec_blks.0.layerNormalizationBlocks.1.scale          256         True               (256,)\n",
      "decoder.dec_blks.0.layerNormalizationBlocks.1.shift          256         True               (256,)\n",
      "decoder.dec_blks.0.layerNormalizationBlocks.2.scale          256         True               (256,)\n",
      "decoder.dec_blks.0.layerNormalizationBlocks.2.shift          256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block1.w_q.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block1.w_q.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block1.w_k.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block1.w_k.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block1.w_v.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block1.w_v.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block1.w_o.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block1.w_o.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block2.w_q.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block2.w_q.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block2.w_k.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block2.w_k.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block2.w_v.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block2.w_v.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.mha_block2.w_o.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.1.mha_block2.w_o.bias            256         True               (256,)\n",
      "decoder.dec_blks.1.feed_forward_block.network.0.weight      262,144         True          (1024, 256)\n",
      "decoder.dec_blks.1.feed_forward_block.network.0.bias        1,024         True              (1024,)\n",
      "decoder.dec_blks.1.feed_forward_block.network.3.weight      262,144         True          (256, 1024)\n",
      "decoder.dec_blks.1.feed_forward_block.network.3.bias          256         True               (256,)\n",
      "decoder.dec_blks.1.layerNormalizationBlocks.0.scale          256         True               (256,)\n",
      "decoder.dec_blks.1.layerNormalizationBlocks.0.shift          256         True               (256,)\n",
      "decoder.dec_blks.1.layerNormalizationBlocks.1.scale          256         True               (256,)\n",
      "decoder.dec_blks.1.layerNormalizationBlocks.1.shift          256         True               (256,)\n",
      "decoder.dec_blks.1.layerNormalizationBlocks.2.scale          256         True               (256,)\n",
      "decoder.dec_blks.1.layerNormalizationBlocks.2.shift          256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block1.w_q.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block1.w_q.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block1.w_k.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block1.w_k.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block1.w_v.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block1.w_v.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block1.w_o.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block1.w_o.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block2.w_q.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block2.w_q.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block2.w_k.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block2.w_k.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block2.w_v.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block2.w_v.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.mha_block2.w_o.weight       65,536         True           (256, 256)\n",
      "decoder.dec_blks.2.mha_block2.w_o.bias            256         True               (256,)\n",
      "decoder.dec_blks.2.feed_forward_block.network.0.weight      262,144         True          (1024, 256)\n",
      "decoder.dec_blks.2.feed_forward_block.network.0.bias        1,024         True              (1024,)\n",
      "decoder.dec_blks.2.feed_forward_block.network.3.weight      262,144         True          (256, 1024)\n",
      "decoder.dec_blks.2.feed_forward_block.network.3.bias          256         True               (256,)\n",
      "decoder.dec_blks.2.layerNormalizationBlocks.0.scale          256         True               (256,)\n",
      "decoder.dec_blks.2.layerNormalizationBlocks.0.shift          256         True               (256,)\n",
      "decoder.dec_blks.2.layerNormalizationBlocks.1.scale          256         True               (256,)\n",
      "decoder.dec_blks.2.layerNormalizationBlocks.1.shift          256         True               (256,)\n",
      "decoder.dec_blks.2.layerNormalizationBlocks.2.scale          256         True               (256,)\n",
      "decoder.dec_blks.2.layerNormalizationBlocks.2.shift          256         True               (256,)\n",
      "decoder_final_projection.linear.weight      7,680,000         True         (30000, 256)\n",
      "decoder_final_projection.linear.bias           30,000         True             (30000,)\n",
      "--------------------------------------------------------------------------------\n",
      "Total Parameters: 28,599,600\n",
      "Trainable Parameters: 28,599,600\n",
      "Non-trainable Parameters: 0\n",
      "Model Size (approx): 109.10 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def model_summary(model):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model Summary':^80}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    print(f\"{'Layer Name':40s} {'Param #':>12s} {'Trainable':>12s} {'Shape':>20s}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        shape = tuple(param.shape)\n",
    "        print(f\"{name:40s} {num_params:12,d} {str(param.requires_grad):>12s} {str(shape):>20s}\")\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable Parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "    total_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    print(f\"Model Size (approx): {total_size_bytes / (1024**2):.2f} MB\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Example usage\n",
    "# model = Transformer(...)\n",
    "model_summary(Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdafb0",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc42d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_prediction_accuracy(predictions,targets):\n",
    "    prediction_tokens=predictions.argmax(dim=-1)\n",
    "    mask=targets!=tokenizer_fr.token_to_id('[PAD]')\n",
    "    correct=(prediction_tokens==targets) & mask\n",
    "    acc=correct.sum().float()/mask.sum().float()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8c26db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/40667 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m encoder_mask=batch[\u001b[33m'\u001b[39m\u001b[33mencoder_mask\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     20\u001b[39m decoder_mask=batch[\u001b[33m'\u001b[39m\u001b[33mdecoder_mask\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m decoder_output=\u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m acc=get_prediction_accuracy(decoder_output,target_output)\n\u001b[32m     24\u001b[39m train_acc+=acc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtransformers.forward\u001b[39m\u001b[34m(self, decoder_input, encoder_input, decoder_mask, encoder_mask)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,decoder_input,encoder_input,decoder_mask,encoder_mask):\n\u001b[32m     44\u001b[39m     encoder_output=\u001b[38;5;28mself\u001b[39m.encode(encoder_input,encoder_mask)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     decoder_output=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_output\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtransformers.decode\u001b[39m\u001b[34m(self, decoder_input, encoder_output, decoder_mask, encoder_mask)\u001b[39m\n\u001b[32m     36\u001b[39m decoder_input_embedding=\u001b[38;5;28mself\u001b[39m.decoder_emb_layer(decoder_input)\n\u001b[32m     37\u001b[39m positional_encoded_input_embedding=\u001b[38;5;28mself\u001b[39m.dec_positional_emb_layer(decoder_input_embedding) \u001b[38;5;66;03m### of decoder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m decoder_contexual_embedding=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositional_encoded_input_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m final_output=\u001b[38;5;28mself\u001b[39m.decoder_final_projection(decoder_contexual_embedding)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m final_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mdecoder.forward\u001b[39m\u001b[34m(self, x, encoder_output, decoder_mask, encoder_mask)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,encoder_output,decoder_mask=\u001b[38;5;28;01mNone\u001b[39;00m,encoder_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dec_blks:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         x=\u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mdecoderBlock.forward\u001b[39m\u001b[34m(self, x, enc_out, decoder_mask, encoder_mask)\u001b[39m\n\u001b[32m     16\u001b[39m output1=\u001b[38;5;28mself\u001b[39m.skip_connections[\u001b[32m0\u001b[39m](x,\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m.mha_block1(x,x,x,decoder_mask)) \u001b[38;5;66;03m### causal attention\u001b[39;00m\n\u001b[32m     17\u001b[39m layer_normalized_output1=\u001b[38;5;28mself\u001b[39m.layerNormalizationBlocks[\u001b[32m0\u001b[39m](output1)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m output2=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mskip_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_normalized_output1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmha_block2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m### Query from decoder, k and v from encoder.\u001b[39;00m\n\u001b[32m     19\u001b[39m layer_normalized_output2=\u001b[38;5;28mself\u001b[39m.layerNormalizationBlocks[\u001b[32m1\u001b[39m](output2)\n\u001b[32m     20\u001b[39m output3=\u001b[38;5;28mself\u001b[39m.skip_connections[\u001b[32m2\u001b[39m](layer_normalized_output2,\u001b[38;5;28mself\u001b[39m.feed_forward_block)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mskipConnection.forward\u001b[39m\u001b[34m(self, x, sublayer)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,sublayer):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     output=x+\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     dropped_output=\u001b[38;5;28mself\u001b[39m.dropout(output)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dropped_output\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mdecoderBlock.forward.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     16\u001b[39m output1=\u001b[38;5;28mself\u001b[39m.skip_connections[\u001b[32m0\u001b[39m](x,\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m.mha_block1(x,x,x,decoder_mask)) \u001b[38;5;66;03m### causal attention\u001b[39;00m\n\u001b[32m     17\u001b[39m layer_normalized_output1=\u001b[38;5;28mself\u001b[39m.layerNormalizationBlocks[\u001b[32m0\u001b[39m](output1)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m output2=\u001b[38;5;28mself\u001b[39m.skip_connections[\u001b[32m1\u001b[39m](layer_normalized_output1,\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmha_block2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m### Query from decoder, k and v from encoder.\u001b[39;00m\n\u001b[32m     19\u001b[39m layer_normalized_output2=\u001b[38;5;28mself\u001b[39m.layerNormalizationBlocks[\u001b[32m1\u001b[39m](output2)\n\u001b[32m     20\u001b[39m output3=\u001b[38;5;28mself\u001b[39m.skip_connections[\u001b[32m2\u001b[39m](layer_normalized_output2,\u001b[38;5;28mself\u001b[39m.feed_forward_block)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mmultiHeadAttentionBlock.forward\u001b[39m\u001b[34m(self, q, k, v, mask)\u001b[39m\n\u001b[32m     38\u001b[39m final_contextual_embeddings=contextual_embeddings.transpose(-\u001b[32m3\u001b[39m,-\u001b[32m2\u001b[39m).contiguous().view(query.shape[\u001b[32m0\u001b[39m],query.shape[\u001b[32m1\u001b[39m],\u001b[38;5;28mself\u001b[39m.n_heads*\u001b[38;5;28mself\u001b[39m.single_head_dim) \u001b[38;5;66;03m### \u001b[39;00m\n\u001b[32m     39\u001b[39m multihead_final_contextual_embedding_proj=\u001b[38;5;28mself\u001b[39m.w_o(final_contextual_embeddings)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m dropped_multihead_final_contextual_embedding_proj=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultihead_final_contextual_embedding_proj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dropped_multihead_final_contextual_embedding_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer_fr.token_to_id('[PAD]'),label_smoothing=0.1)\n",
    "optimizer=optim.Adam(Transformer.parameters(),lr=1e-5,eps=1e-8)\n",
    "epochs=3\n",
    "global_step=0\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ### Train cycle for each epcoh, loss calculation. Train loss.\n",
    "    Transformer.train()\n",
    "    train_loss=0.0\n",
    "    train_acc=0.0\n",
    "    train_loader_tqdm=tqdm(train_loader,desc=\"Training Epoch\",leave=False)\n",
    "    for i,batch in enumerate(train_loader_tqdm):\n",
    "        encoder_input=batch['encoder_input']\n",
    "        decoder_input=batch['decoder_input']\n",
    "        target_output=batch['target_output']\n",
    "        encoder_mask=batch['encoder_mask']\n",
    "        decoder_mask=batch['decoder_mask']\n",
    "        decoder_output=Transformer(decoder_input,encoder_input,decoder_mask,encoder_mask)\n",
    "        \n",
    "        acc=get_prediction_accuracy(decoder_output,target_output)\n",
    "        train_acc+=acc\n",
    "        loss=loss_fn(decoder_output.reshape(-1,decoder_output.shape[-1]),target_output.reshape(-1))\n",
    "        train_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loader_tqdm.set_postfix({f\"{i+1}th Train batch loss: \":loss.item(),f\"{i+1}th Train batch acc: \":acc.item()})\n",
    "        \n",
    "        ###adding below step to quick check entire loop.\n",
    "        # global_step+=1\n",
    "        # if global_step>=1:\n",
    "        #     break    \n",
    "\n",
    "    train_loss/=len(train_loader)\n",
    "    train_acc/=len(train_loader)\n",
    "    ### for val loss\n",
    "    validation_loss=0.0\n",
    "    validation_acc=0.0\n",
    "    with torch.no_grad():\n",
    "        Transformer.eval()\n",
    "        validation_loader_tqdm=tqdm(val_loader,desc=\"Validation Epoch\",leave=False)\n",
    "        for i,batch in enumerate(validation_loader_tqdm):\n",
    "            encoder_input=batch['encoder_input']\n",
    "            decoder_input=batch['decoder_input']\n",
    "            target_output=batch['target_output']\n",
    "            encoder_mask=batch['encoder_mask']\n",
    "            decoder_mask=batch['decoder_mask']\n",
    "            decoder_output=Transformer(decoder_input,encoder_input,decoder_mask,encoder_mask)\n",
    "             \n",
    "            acc=get_prediction_accuracy(decoder_output,target_output)\n",
    "            validation_acc+=acc\n",
    "\n",
    "            loss=loss_fn(decoder_output.reshape(-1,decoder_output.shape[-1]),target_output.reshape(-1))\n",
    "            validation_loss+=loss.item()\n",
    "            validation_loader_tqdm.set_postfix({f\"{i+1}th validation batch loss: \":loss.item(),f\"{i+1}th validation batch acc: \":acc.item()})\n",
    "            \n",
    "            # if global_step>=1:\n",
    "            #     break\n",
    "    validation_loss/=len(val_loader)\n",
    "    validation_acc/=len(val_loader)\n",
    "    print(f\"Epoch: {epoch+1} | Train loss: {train_loss} | Train acc: {train_acc} | Validation loss: {validation_loss} | Validation acc: {validation_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d2e15de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_preprocess(text):\n",
    "    encoded_english_sentence=tokenizer_en.encode(text).ids\n",
    "    # print(tokenizer_en.decode(encoded_english_sentence))\n",
    "    processed_text=torch.cat(\n",
    "        [\n",
    "            torch.tensor(encoded_english_sentence,dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_en.token_to_id('[EOS]')],dtype=torch.int64),\n",
    "        ]\n",
    "    )\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ee2f4",
   "metadata": {},
   "source": [
    "### inference pipeline for 1 sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68d0af04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "tensor([[2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]])\n",
      "[SOS] , , , , , , , , , ,\n"
     ]
    }
   ],
   "source": [
    "### For single input\n",
    "with torch.inference_mode():\n",
    "    max_seq_len=432\n",
    "    english_sentence=\"I am who I am\"\n",
    "    processed_encoded_english_sentence=encoder_preprocess(english_sentence).unsqueeze(0)\n",
    "    # print(processed_encoded_english_sentence.shape) ### batch, seq\n",
    "    encoder_mask=(processed_encoded_english_sentence!=tokenizer_en.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int()\n",
    "    # print(encoder_mask.shape) #batch,head,seq,dim\n",
    "    encoder_output=Transformer.encode(processed_encoded_english_sentence,encoder_mask)\n",
    "    # # print(encoder_output.shape)\n",
    "    decoder_input=torch.tensor([tokenizer_fr.encode(\"[SOS]\").ids])  #seq\n",
    "    # print(decoder_input.shape)\n",
    "    i=0\n",
    "    while True:\n",
    "\n",
    "        if decoder_input.shape[1]>=max_seq_len:\n",
    "            break\n",
    "\n",
    "        decoder_mask= ((decoder_input!=torch.tensor(tokenizer_fr.token_to_id(\"[PAD]\"))).unsqueeze(0).unsqueeze(0)) & casual_mask_generator(decoder_input.shape[1])\n",
    "\n",
    "        # print(decoder_input.shape,decoder_mask.shape)\n",
    "        decoder_output=Transformer.decode(decoder_input,encoder_output,decoder_mask,encoder_mask)\n",
    "        ##batch,seq,vocab\n",
    "        # print(decoder_output.shape)\n",
    "        predicted_word=torch.argmax(decoder_output[:,-1,:],dim=-1)\n",
    "\n",
    "        decoder_input=torch.cat(\n",
    "            [\n",
    "                decoder_input.squeeze(0),\n",
    "                torch.tensor([predicted_word])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # print(decoder_input)\n",
    "        decoder_input=decoder_input.unsqueeze(0)\n",
    "\n",
    "        if predicted_word==tokenizer_fr.token_to_id(\"[EOS]\") or predicted_word==tokenizer_fr.token_to_id(\"[PAD]\"):\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "        i+=1\n",
    "        if i==10:\n",
    "            break\n",
    "\n",
    "    predicted_sentence=tokenizer_fr.decode(decoder_input.squeeze(0).tolist(),skip_special_tokens=False)\n",
    "    print(decoder_input)\n",
    "    print(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64477818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
