{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac3d5d0",
   "metadata": {},
   "source": [
    "<h1>Data Ingestion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "788655d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "### Using the Opus Books Dataset from Huggingface\n",
    "def data_ingestion():\n",
    "    ds = load_dataset(path=\"Helsinki-NLP/opus_books\", name=\"en-fr\")\n",
    "    train_test_data=ds['train'].train_test_split(test_size=0.2,seed=42)\n",
    "    test_data=train_test_data['test']\n",
    "    train_val_split=train_test_data['train'].train_test_split(test_size=0.2,seed=42)\n",
    "    train_data=train_val_split['train']\n",
    "    validation_data=train_val_split['test']\n",
    "    return train_data,validation_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "978bfd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,validation_data,test_data=data_ingestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9558298",
   "metadata": {},
   "source": [
    "<b> creating a data generator function, it will be needed when building tokenizer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc99b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "    for pair in ds:\n",
    "        # print(pair)\n",
    "        yield pair['translation'][lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74944ecf",
   "metadata": {},
   "source": [
    "<h5>Building Tokenizer</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75646a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "def build_tokenizer(config,ds,lang):\n",
    "    tokenizer_path=Path(config['tokenizer_file'].format(lang))\n",
    "    \n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        trainer=WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],min_frequency=1)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4370b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THe tokenizer will be trained on the train set of the data only.\n",
    "##There will be 2 separate tokenizer, one will be for english and other will be for french.\n",
    "tokenizer_en=build_tokenizer({'tokenizer_file':'tokenizer_en.json'},train_data,'en')\n",
    "tokenizer_fr=build_tokenizer({'tokenizer_file':'tokenizer_fr.json'},train_data,'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8415b8",
   "metadata": {},
   "source": [
    "### creating max sequence len utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6929639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to get the max seq len that is possible in the dataset.\n",
    "def get_max_seq_len(train_data,test_data,validation_data):\n",
    "    max_len=0\n",
    "    for data in train_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "\n",
    "    for data in test_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "\n",
    "    for data in validation_data:\n",
    "        max_len=max(max_len,len(tokenizer_en.encode(data['translation']['en']).ids))\n",
    "        max_len=max(max_len,len(tokenizer_fr.encode(data['translation']['fr']).ids))\n",
    "    return max_len    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d6cea",
   "metadata": {},
   "source": [
    "### Causal Mask Generator Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac0c3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def casual_mask_generator(size):\n",
    "    mask=torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int)\n",
    "    return mask==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6f3c7",
   "metadata": {},
   "source": [
    "### Creating the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a9e29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class opusDataset_En_to_Fr(Dataset):\n",
    "    def __init__(self,data,max_seq_len,tokenizer_en,tokenizer_fr):\n",
    "        super().__init__()\n",
    "        self.raw_data=data\n",
    "        self.tokenizer_en=tokenizer_en\n",
    "        self.tokenizer_fr=tokenizer_fr\n",
    "\n",
    "\n",
    "        ### Goal Shoould be to set a max length that fits all the sequence..\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.sos_token=torch.tensor([self.tokenizer_en.token_to_id(\"[SOS]\")],dtype=torch.int64)\n",
    "        self.eos_token=torch.tensor([self.tokenizer_en.token_to_id(\"[EOS]\")],dtype=torch.int64)\n",
    "        self.pad_token=torch.tensor([self.tokenizer_en.token_to_id(\"[PAD]\")],dtype=torch.int64)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_en=self.raw_data[index]['translation']['en']\n",
    "        data_fr=self.raw_data[index]['translation']['fr']\n",
    "        encoded_data_en=torch.tensor(self.tokenizer_en.encode(data_en).ids,dtype=torch.int64)\n",
    "        encoded_data_fr=torch.tensor(self.tokenizer_fr.encode(data_fr).ids,dtype=torch.int64)\n",
    "        expected_seq_len=self.max_seq_len+2 ### +2 for sos and eos token\n",
    "\n",
    "        final_encoded_en=torch.cat([\n",
    "            self.sos_token,\n",
    "            encoded_data_en,\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token]*(expected_seq_len-len(encoded_data_en)-2)),\n",
    "\n",
    "        ]\n",
    "        )\n",
    "        final_encoded_fr=torch.cat([\n",
    "            self.sos_token,\n",
    "            encoded_data_fr,\n",
    "            torch.tensor([self.pad_token]*(expected_seq_len-len(encoded_data_fr)-1))\n",
    "        ])\n",
    "\n",
    "\n",
    "        target_encoded_fr=torch.cat([\n",
    "            self.sos_token,\n",
    "            encoded_data_fr,\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token]*(expected_seq_len-len(encoded_data_fr)-2)),\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'encoder_input':final_encoded_en,\n",
    "            'decoder_input':final_encoded_fr,\n",
    "            'target_output':target_encoded_fr,\n",
    "            'encoder_mask': (final_encoded_en!=self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decodar_mask': (target_encoded_fr!=self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask_generator(len(target_encoded_fr)),\n",
    "            'src_sentance':data_en,\n",
    "            'tgt_sentence':data_fr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4631c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=get_max_seq_len(train_data,validation_data,test_data) ### using the utility function that we created earlier.\n",
    "train_dataset=opusDataset_En_to_Fr(train_data,max_seq_len,tokenizer_en,tokenizer_fr)\n",
    "test_dataset=opusDataset_En_to_Fr(test_data,max_seq_len,tokenizer_en,tokenizer_fr)\n",
    "validation_dataset=opusDataset_En_to_Fr(validation_data,max_seq_len,tokenizer_en,tokenizer_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee00c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader=DataLoader(train_dataset,batch_size=2,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=2,shuffle=False)\n",
    "val_loader=DataLoader(validation_dataset,batch_size=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff06f86",
   "metadata": {},
   "source": [
    "### Coding the Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed667c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0be8af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.emb_dim=emb_dim\n",
    "        self.embedding_layer=nn.Embedding(self.vocab_size,self.emb_dim,dtype=torch.float16)\n",
    "    def forward(self,x):\n",
    "        embeddings=self.embedding_layer(x)\n",
    "        return embeddings*math.sqrt(self.emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80d13660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncodingLayer(nn.Module):\n",
    "    def __init__(self,max_seq_len,emb_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        static_positional_info=torch.zeros((self.max_seq_len,self.emb_dim),dtype=torch.float16)\n",
    "        positions=torch.arange(0,max_seq_len,dtype=torch.float16).reshape(max_seq_len,-1)\n",
    "        indices_for_denominator=torch.arange(0,emb_dim,2,dtype=torch.float16) ### 2i\n",
    "        denominators=torch.exp((-2*indices_for_denominator*math.log(1e4))/emb_dim)\n",
    "        static_positional_info[:,0::2]=torch.sin(positions*denominators)\n",
    "        static_positional_info[:,1::2]=torch.cos(positions*denominators)\n",
    "        self.register_buffer('static_positional_info',static_positional_info)\n",
    "    def forward(self,x):\n",
    "        position_encoded_embedding=x+self.static_positional_info[:x.shape[1],:]\n",
    "        dropped_embeddings=self.dropout(position_encoded_embedding)\n",
    "        return dropped_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "939ce14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,dropout):\n",
    "        super().__init__()\n",
    "        assert emb_dim%n_heads==0 ### checking if multi head splitting is possible.\n",
    "        self.w_q=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_k=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_v=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_o=nn.Linear(emb_dim,emb_dim,dtype=torch.float16) ### multi-head-projection-layer\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.single_head_dim=self.emb_dim//n_heads\n",
    "        self.n_heads=n_heads\n",
    "\n",
    "    @staticmethod\n",
    "    def contextual_embedding(m_q,m_k,m_v,per_head_emb_dim,mask=None,dropout=None):\n",
    "        ### return contexual embedding and attention scores\n",
    "        attention_scores=m_q@m_k.transpose(2,3)/math.sqrt(per_head_emb_dim)\n",
    "        ##batch,head,seq,dim @ batch,head,dim,seq==batch,head,seq,seq\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask==0,value=float('-inf'))\n",
    "        normalized_attention_scores=torch.softmax(attention_scores,dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            dropped_normalized_attention_scores=dropout(normalized_attention_scores)\n",
    "        ### batch,head,seq,seq @ batch,head,seq,dim=batch,head,seq,dim\n",
    "        contexual_embeddings=dropped_normalized_attention_scores@m_v\n",
    "        return dropped_normalized_attention_scores,contexual_embeddings\n",
    "    \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        query=self.w_q(q) ### batch, seqeunce, dim\n",
    "        key=self.w_k(k)\n",
    "        value=self.w_v(v)\n",
    "\n",
    "        multihead_query=query.view(query.shape[0],query.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        multihead_key=key.view(key.shape[0],key.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        multihead_value=value.view(value.shape[0],value.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        _,contextual_embeddings=multiHeadAttentionBlock.contextual_embedding(multihead_query,multihead_key,multihead_value,self.single_head_dim,mask,self.dropout)\n",
    "        final_contextual_embeddings=contextual_embeddings.transpose(1,2).contiguous().view(value.shape[0],value.shape[1],self.n_heads*self.single_head_dim)\n",
    "        multihead_final_contextual_embedding_proj=self.w_o(final_contextual_embeddings)\n",
    "        dropped_multihead_final_contextual_embedding_proj=self.dropout(multihead_final_contextual_embedding_proj)\n",
    "        return dropped_multihead_final_contextual_embedding_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f341cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerNormalizationBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim,dtype=torch.float16))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim,dtype=torch.float16))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        standard_deviation=x.std(dim=-1,keepdim=True,unbiased=False)\n",
    "        normalized_x=(x-mean)/(standard_deviation+self.eps)\n",
    "        scale_n_shift=self.scale*normalized_x+self.shift\n",
    "        return scale_n_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3794f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipConnection(nn.Module):\n",
    "    def __init__(self,dropout):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        output=x+sublayer(x)\n",
    "        dropped_output=self.dropout(output)\n",
    "        return dropped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b52d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward_block(nn.Module):\n",
    "    ### Expansion Contraction layer.....\n",
    "    def __init__(self,emb_dim,expand_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.expand_dim=expand_dim\n",
    "        self.dropout=dropout\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(emb_dim,expand_dim,dtype=torch.float16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(expand_dim,emb_dim,dtype=torch.float16),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        output=self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30009fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.ff_dropout=ff_dropout\n",
    "        self.expand_dim=expand_dim\n",
    "        self.mha_dropout=mha_dropout\n",
    "        self.sk_dropout=sk_dropout\n",
    "        self.mha_block=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout)\n",
    "        self.feed_forward_block=feed_forward_block(self.emb_dim,self.expand_dim,self.ff_dropout)\n",
    "        self.skip_connections=nn.ModuleList([skipConnection(self.sk_dropout) for _ in range(2)])\n",
    "        self.layerNormalizationBlocks=nn.ModuleList([layerNormalizationBlock(self.emb_dim) for _ in range(2)])\n",
    "    def forward(self,x,encoder_mask=None):\n",
    "        output1=self.skip_connections[0](x,lambda x: self.mha_block(x,x,x,encoder_mask))\n",
    "        layer_normalized_output1=self.layerNormalizationBlocks[0](output1)\n",
    "        output2=self.skip_connections[1](layer_normalized_output1,self.feed_forward_block)\n",
    "        layer_normalized_output2=self.layerNormalizationBlocks[1](output2)\n",
    "        return layer_normalized_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ed48411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,no_of_enc_blk,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.enc_blks=nn.ModuleList([encoderBlock(emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout) for _ in range(no_of_enc_blk)])\n",
    "    def forward(self,x,encoder_mask=None):\n",
    "        for blk in self.enc_blks:\n",
    "            x=blk(x,encoder_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f93ea5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.ff_dropout=ff_dropout\n",
    "        self.expand_dim=expand_dim\n",
    "        self.mha_dropout=mha_dropout\n",
    "        self.sk_dropout=sk_dropout\n",
    "        self.mha_block1=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout) ### casual attention block\n",
    "        self.mha_block2=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout) #### cross attention block\n",
    "        self.feed_forward_block=feed_forward_block(self.emb_dim,self.expand_dim,self.ff_dropout)\n",
    "        self.skip_connections=nn.ModuleList([skipConnection(self.sk_dropout) for _ in range(3)])\n",
    "        self.layerNormalizationBlocks=nn.ModuleList([layerNormalizationBlock(self.emb_dim) for _ in range(3)])\n",
    "    def forward(self,x,enc_out,decoder_mask=None,encoder_mask=None):\n",
    "        output1=self.skip_connections[0](x,lambda x: self.mha_block1(x,x,x,decoder_mask)) ### causal attention\n",
    "        layer_normalized_output1=self.layerNormalizationBlocks[0](output1)\n",
    "        output2=self.skip_connections[1](layer_normalized_output1,lambda x: self.mha_block2(x,enc_out,enc_out,encoder_mask))### Query from decoder, k and v from encoder.\n",
    "        layer_normalized_output2=self.layerNormalizationBlocks[1](output2)\n",
    "        output3=self.skip_connections[2](layer_normalized_output2,self.feed_forward_block)\n",
    "        layer_normalized_output3=self.layerNormalizationBlocks[2](output3)\n",
    "        return layer_normalized_output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0adfb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self,no_of_dec_blk,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.dec_blks=nn.ModuleList([decoderBlock(emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout) for _ in range(no_of_dec_blk)])\n",
    "    def forward(self,x,encoder_output,decoder_mask=None,encoder_mask=None):\n",
    "        for blk in self.dec_blks:\n",
    "            x=blk(x,encoder_output,decoder_mask,encoder_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92b29dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class finalProjectionLayer(nn.Module):\n",
    "    def __init__(self,emb_dim,vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(emb_dim,vocab_size,dtype=torch.float16)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output=self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08b95afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformers(nn.Module):\n",
    "    def __init__(self,model_config,tokenizer_config):\n",
    "        super().__init__()\n",
    "        self.encoder_emb_layer=inputEmbeddingLayer(tokenizer_config['en_vocab_size'],model_config['enc_cfg']['emb_dim'])\n",
    "        self.enc_positional_emb_layer=positionalEncodingLayer(model_config['enc_max_seq_len'],model_config['enc_cfg']['emb_dim'],model_config['enc_cfg']['pos_emb_dropout'])\n",
    "        self.encoder=encoder(\n",
    "            no_of_enc_blk=model_config['enc_cfg']['no_of_enc_blk'],\n",
    "            emb_dim=model_config['enc_cfg']['emb_dim'],\n",
    "            n_heads=model_config['enc_cfg']['n_heads'],\n",
    "            mha_dropout=model_config['enc_cfg']['mha_dropout'],\n",
    "            expand_dim=model_config['enc_cfg']['expand_dim'],\n",
    "            ff_dropout=model_config['enc_cfg']['ff_dropout'],\n",
    "            sk_dropout=model_config['enc_cfg']['sk_dropout']\n",
    "        )\n",
    "        \n",
    "        self.decoder_emb_layer=inputEmbeddingLayer(tokenizer_config['fr_vocab_size'],model_config['dec_cfg']['emb_dim'])\n",
    "        self.dec_positional_emb_layer=positionalEncodingLayer(model_config['dec_max_seq_len'],model_config['dec_cfg']['emb_dim'],model_config['dec_cfg']['pos_emb_dropout'])\n",
    "        self.decoder=decoder(\n",
    "            no_of_dec_blk=model_config['dec_cfg']['no_of_dec_blk'],\n",
    "            emb_dim=model_config['dec_cfg']['emb_dim'],\n",
    "            n_heads=model_config['dec_cfg']['n_heads'],\n",
    "            mha_dropout=model_config['dec_cfg']['mha_dropout'],\n",
    "            expand_dim=model_config['dec_cfg']['expand_dim'],\n",
    "            ff_dropout=model_config['dec_cfg']['ff_dropout'],\n",
    "            sk_dropout=model_config['dec_cfg']['sk_dropout']\n",
    "        )\n",
    "        self.decoder_final_projection=finalProjectionLayer(model_config['dec_cfg']['emb_dim'],tokenizer_config['fr_vocab_size'])\n",
    "\n",
    "    def encode(self,encoder_input,encoder_mask=None):\n",
    "        encoder_input_embedding=self.encoder_emb_layer(encoder_input)\n",
    "        positional_encoded_input_embedding=self.enc_positional_emb_layer(encoder_input_embedding)\n",
    "        encoder_contexual_embedding=self.encoder(positional_encoded_input_embedding,encoder_mask)\n",
    "        return encoder_contexual_embedding\n",
    "\n",
    "    def decode(self,decoder_input,encoder_output,decoder_mask,encoder_mask):\n",
    "        decoder_input_embedding=self.decoder_emb_layer(decoder_input)\n",
    "        positional_encoded_input_embedding=self.dec_positional_emb_layer(decoder_input_embedding) ### of decoder\n",
    "        decoder_contexual_embedding=self.decoder(positional_encoded_input_embedding,encoder_output,decoder_mask,encoder_mask)\n",
    "        final_output=self.decoder_final_projection(decoder_contexual_embedding)\n",
    "        return final_output\n",
    "    \n",
    "    ###forward will be used during training.\n",
    "    def forward(self,decoder_input,encoder_input,decoder_mask,encoder_mask):\n",
    "        encoder_output=self.encode(encoder_input,encoder_mask)\n",
    "        decoder_output=self.decode(decoder_input,encoder_output,decoder_mask,encoder_mask)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a03e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration dictionary\n",
    "model_config = {\n",
    "    \"enc_max_seq_len\": max_seq_len+2,   # Max source sequence length  Extra +2 for start and end token\n",
    "    \"dec_max_seq_len\": max_seq_len+2,   # Max target sequence length\n",
    "    \"enc_cfg\": {\n",
    "        \"emb_dim\": 512,\n",
    "        \"no_of_enc_blk\": 6,\n",
    "        \"n_heads\": 8,\n",
    "        \"pos_emb_dropout\":0.1,\n",
    "        \"mha_dropout\": 0.1,\n",
    "        \"expand_dim\": 2048,\n",
    "        \"ff_dropout\": 0.1,\n",
    "        \"sk_dropout\": 0.1\n",
    "    },\n",
    "    \"dec_cfg\": {\n",
    "        \"emb_dim\": 512, ### Embedding dimention\n",
    "        \"no_of_dec_blk\": 6, ## No of decoder block\n",
    "        \"pos_emb_dropout\":0.1, # dropout rate after applying  positional embedding.\n",
    "        \"n_heads\": 8, ### no of head\n",
    "        \"mha_dropout\": 0.1, ### multihead attentions dropout in attention scores\n",
    "        \"expand_dim\": 2048, ### fee forward intermediate expand dim\n",
    "        \"ff_dropout\": 0.1, ### feed forward layer dropout\n",
    "        \"sk_dropout\": 0.1 ##skip connections dropout\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tokenizer configuration dictionary\n",
    "tokenizer_config = {\n",
    "    # \"vocab_size\": 30000,  # Vocabulary size of source & target tokenizer\n",
    "    'en_vocab_size':tokenizer_en.get_vocab_size(),\n",
    "    'fr_vocab_size':tokenizer_fr.get_vocab_size()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "164bdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer=transformers(model_config,tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3845d24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers(\n",
       "  (encoder_emb_layer): inputEmbeddingLayer(\n",
       "    (embedding_layer): Embedding(30000, 512)\n",
       "  )\n",
       "  (enc_positional_emb_layer): positionalEncodingLayer(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): encoder(\n",
       "    (enc_blks): ModuleList(\n",
       "      (0-5): 6 x encoderBlock(\n",
       "        (mha_block): multiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): feed_forward_block(\n",
       "          (network): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (skip_connections): ModuleList(\n",
       "          (0-1): 2 x skipConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layerNormalizationBlocks): ModuleList(\n",
       "          (0-1): 2 x layerNormalizationBlock()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_emb_layer): inputEmbeddingLayer(\n",
       "    (embedding_layer): Embedding(30000, 512)\n",
       "  )\n",
       "  (dec_positional_emb_layer): positionalEncodingLayer(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): decoder(\n",
       "    (dec_blks): ModuleList(\n",
       "      (0-5): 6 x decoderBlock(\n",
       "        (mha_block1): multiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mha_block2): multiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): feed_forward_block(\n",
       "          (network): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (skip_connections): ModuleList(\n",
       "          (0-2): 3 x skipConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layerNormalizationBlocks): ModuleList(\n",
       "          (0-2): 3 x layerNormalizationBlock()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_final_projection): finalProjectionLayer(\n",
       "    (linear): Linear(in_features=512, out_features=30000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ff6f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 484, 30000])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    encoder_input=batch['encoder_input']\n",
    "    decoder_input=batch['decoder_input']\n",
    "    target_output=batch['target_output']\n",
    "    encoder_mask=batch['encoder_mask']\n",
    "    decoder_mask=batch['decodar_mask']\n",
    "\n",
    "    decoder_output=Transformer(decoder_input,encoder_input,decoder_mask,encoder_mask)\n",
    "    print(decoder_output.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feffdb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06a510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
