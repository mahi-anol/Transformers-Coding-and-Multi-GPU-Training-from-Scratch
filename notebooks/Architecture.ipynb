{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b73e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca648c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1088ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91f0b1",
   "metadata": {},
   "source": [
    "### Coding the input Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "652e4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputEmebeddingLayer(nn.Module):\n",
    "    ### vocab_size contains the vocab size and d_model contains the embedding dimention that we expect to create with this layer.\n",
    "    def __init__(self,vocab_size:int,d_model:int):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model ### Expected dimention \n",
    "        self.vocab_size=vocab_size ### models vocabulary.\n",
    "        self.embedding=nn.Embedding(self.vocab_size,self.d_model) ### rowwise vocability and column wise expectected dimentions for vector.\n",
    "    def forward(self,input_tokens):\n",
    "        ### recieves inputs of fixed context length\n",
    "        ### The input tokens will be in [int] representation.\n",
    "        return self.embedding(input_tokens) * math.sqrt(self.d_model) ## multiplied for neumerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d96e63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding_layer=inputEmebeddingLayer(vocab_size=11,d_model=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7ef39aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.6439e-04,  3.2745e+00, -1.7380e+00, -9.8305e-01],\n",
       "         [ 2.2389e+00, -5.7690e-01, -3.3584e+00, -3.0447e+00],\n",
       "         [ 1.2606e+00,  1.3314e+00,  2.3327e+00, -2.1338e-01],\n",
       "         [ 3.8559e+00, -2.9000e-01,  1.6574e+00,  8.4139e-01]],\n",
       "\n",
       "        [[ 1.2606e+00,  1.3314e+00,  2.3327e+00, -2.1338e-01],\n",
       "         [ 3.8559e+00, -2.9000e-01,  1.6574e+00,  8.4139e-01],\n",
       "         [ 7.8143e-01,  1.2731e+00,  9.7494e-01,  2.3673e+00],\n",
       "         [ 7.8143e-01,  1.2731e+00,  9.7494e-01,  2.3673e+00]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings=input_embedding_layer(torch.tensor([[1,2,3,4],[3,4,5,5]])) ### sending a batch of 2.\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4169ee",
   "metadata": {},
   "source": [
    "### Coding the positional Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEmbeddings(nn.Module):\n",
    "    ### max_seq_len is the context window here.\n",
    "    def __init__(self, max_seq_len:int,d_model:int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.d_model=d_model\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        static_positional_embedding=torch.zeros(self.max_seq_len,self.d_model) ### initializing with zeros\n",
    "        pairs=torch.arange(0,d_model,2).float() #indice pairs.\n",
    "        pairwise_denominator=torch.exp(pairs*(-math.log(1e4)/self.d_model))\n",
    "        positions=torch.arange(0,max_seq_len,dtype=torch.float).unsqueeze(1)\n",
    "        static_positional_embedding[0:,0::2]=torch.sin(positions*pairwise_denominator)\n",
    "        static_positional_embedding[0:,1::2]=torch.cos(positions*pairwise_denominator)\n",
    "        static_positional_embedding=static_positional_embedding.unsqueeze(0)\n",
    "        self.register_buffer('pe',static_positional_embedding)\n",
    "    def forward(self,inputEmebeddings):\n",
    "        output=input_embeddings+(self.pe[:,:inputEmebeddings.shape[1],:])\n",
    "        return self.dropout(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e141e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=positionalEmbeddings(max_seq_len=11,d_model=4,dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c8962030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.6439e-04,  3.2745e+00, -1.7380e+00, -9.8305e-01],\n",
      "         [ 2.2389e+00, -5.7690e-01, -3.3584e+00, -3.0447e+00],\n",
      "         [ 1.2606e+00,  1.3314e+00,  2.3327e+00, -2.1338e-01],\n",
      "         [ 3.8559e+00, -2.9000e-01,  1.6574e+00,  8.4139e-01]],\n",
      "\n",
      "        [[ 1.2606e+00,  1.3314e+00,  2.3327e+00, -2.1338e-01],\n",
      "         [ 3.8559e+00, -2.9000e-01,  1.6574e+00,  8.4139e-01],\n",
      "         [ 7.8143e-01,  1.2731e+00,  9.7494e-01,  2.3673e+00],\n",
      "         [ 7.8143e-01,  1.2731e+00,  9.7494e-01,  2.3673e+00]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c9c1de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "         [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "         [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "         [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "         [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "         [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
      "         [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
      "         [ 0.4121, -0.9111,  0.0899,  0.9960],\n",
      "         [-0.5440, -0.8391,  0.0998,  0.9950]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3777e-03,  6.1064e+00, -0.0000e+00,  2.4212e-02],\n",
       "         [ 4.4005e+00, -0.0000e+00, -4.7834e+00, -0.0000e+00],\n",
       "         [ 3.0998e+00,  1.3075e+00,  3.3611e+00,  1.1235e+00],\n",
       "         [ 5.7101e+00, -1.8286e+00,  2.4106e+00,  2.6299e+00]],\n",
       "\n",
       "        [[ 1.8008e+00,  3.3306e+00,  3.3325e+00,  1.1237e+00],\n",
       "         [ 6.7106e+00,  0.0000e+00,  2.3820e+00,  0.0000e+00],\n",
       "         [ 2.4153e+00,  1.2242e+00,  1.4213e+00,  4.8102e+00],\n",
       "         [ 1.3179e+00,  4.0444e-01,  1.4356e+00,  0.0000e+00]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e733f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4b68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
