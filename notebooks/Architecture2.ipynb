{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8468ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7ed518e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.emb_dim=emb_dim\n",
    "        self.embedding_layer=nn.Embedding(self.vocab_size,self.emb_dim,dtype=torch.float16)\n",
    "    def forward(self,x):\n",
    "        embeddings=self.embedding_layer(x)\n",
    "        return embeddings*math.sqrt(self.emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f724db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=inputEmbeddingLayer(10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4c0416b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1660,  5.8320, -1.0488, -2.8320,  0.0495, -1.2549],\n",
       "         [-2.5664,  0.9014,  1.9170, -4.7734,  1.9697, -0.9199],\n",
       "         [ 0.1134, -1.7256, -1.6123,  2.3223,  2.4082, -0.6030]],\n",
       "\n",
       "        [[-2.0723, -1.4678,  1.7715, -3.6016,  0.2427, -3.8945],\n",
       "         [ 1.1396,  2.1289, -0.4321,  0.6802,  1.5361, -0.9561],\n",
       "         [-1.2676, -0.5278, -0.1243, -0.2330,  2.8613, -0.7212]]],\n",
       "       dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input=torch.tensor([[1,2,3],[4,5,6]])\n",
    "input_embeddings=embedding_layer(test_input)\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "38abc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncodingLayer(nn.Module):\n",
    "    def __init__(self,max_seq_len,emb_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        static_positional_info=torch.zeros((self.max_seq_len,self.emb_dim),dtype=torch.float16)\n",
    "        positions=torch.arange(0,max_seq_len,dtype=torch.float16).reshape(max_seq_len,-1)\n",
    "        indices_for_denominator=torch.arange(0,emb_dim,2,dtype=torch.float16) ### 2i\n",
    "        denominators=torch.exp((-2*indices_for_denominator*math.log(1e4))/emb_dim)\n",
    "        static_positional_info[:,0::2]=torch.sin(positions*denominators)\n",
    "        static_positional_info[:,1::2]=torch.cos(positions*denominators)\n",
    "        self.register_buffer('static_positional_info',static_positional_info)\n",
    "    def forward(self,x):\n",
    "        position_encoded_embedding=x+self.static_positional_info[:x.shape[1],:]\n",
    "        dropped_embeddings=self.dropout(position_encoded_embedding)\n",
    "        return dropped_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c3326665",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding_layer=positionalEncodingLayer(3,6,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "83f95915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000,  9.7578, -1.4980, -2.6172,  0.0707, -0.3643],\n",
       "         [-2.4648,  0.0000,  2.7422, -5.3906,  2.8145,  0.1144],\n",
       "         [ 1.4609, -3.0605, -2.2988,  4.7461,  3.4414,  0.0000]],\n",
       "\n",
       "        [[-2.9609, -0.6685,  2.5312, -3.7168,  0.3467, -4.1367],\n",
       "         [ 0.0000,  3.8145, -0.0000,  0.0000,  0.0000,  0.0628],\n",
       "         [-0.5122, -1.3496, -0.1714,  1.0957,  4.0898,  0.3984]]],\n",
       "       dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoded_embedding=positional_encoding_layer(input_embeddings)\n",
    "position_encoded_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "01dc91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,dropout):\n",
    "        super().__init__()\n",
    "        assert emb_dim%n_heads==0 ### checking if multi head splitting is possible.\n",
    "        self.w_q=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_k=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_v=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_o=nn.Linear(emb_dim,emb_dim,dtype=torch.float16) ### multi-head-projection-layer\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.single_head_dim=self.emb_dim//n_heads\n",
    "        self.n_heads=n_heads\n",
    "\n",
    "    @staticmethod\n",
    "    def contextual_embedding(m_q,m_k,m_v,per_head_emb_dim,mask):\n",
    "        ### return contexual embedding and attention scores\n",
    "        attention_scores=m_q@m_k.transpose(2,3)/math.sqrt(per_head_emb_dim)\n",
    "        ##batch,head,seq,dim @ batch,head,dim,seq==batch,head,seq,seq\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask,value=float('-inf'))\n",
    "        normalized_attention_scores=torch.softmax(attention_scores,dim=-1)\n",
    "        ### batch,head,seq,seq @ batch,head,seq,dim=batch,head,seq,dim\n",
    "        contexual_embeddings=normalized_attention_scores@m_v\n",
    "        return normalized_attention_scores,contexual_embeddings\n",
    "    \n",
    "    def forward(self,q,k,v,mask):\n",
    "        query=self.w_q(q) ### batch, seqeunce, dim\n",
    "        key=self.w_k(k)\n",
    "        value=self.w_v(v)\n",
    "\n",
    "        multihead_query=query.view(query.shape[0],query.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        multihead_key=key.view(key.shape[0],key.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        multihead_value=value.view(value.shape[0],value.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        _,contextual_embeddings=multiHeadAttentionBlock.contextual_embedding(multihead_query,multihead_key,multihead_value,self.single_head_dim,mask)\n",
    "        final_contextual_embeddings=contextual_embeddings.transpose(1,2).contiguous().view(value.shape[0],value.shape[1],self.n_heads*self.single_head_dim)\n",
    "        multihead_final_contextual_embedding_proj=self.w_o(final_contextual_embeddings)\n",
    "        dropped_multihead_final_contextual_embedding_proj=self.dropout(multihead_final_contextual_embedding_proj)\n",
    "        return dropped_multihead_final_contextual_embedding_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "399b28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mlab= multiHeadAttentionBlock(6,2,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ecd1facc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4644, -1.9199,  0.2600,  0.8218,  1.6543, -0.0000],\n",
       "         [-0.1791, -1.7666,  0.6660,  0.7456,  1.8135,  0.0154],\n",
       "         [-2.3477,  1.7959,  0.0000,  1.1494, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.2021,  0.5527, -0.1633,  0.0631, -0.4094,  0.0034],\n",
       "         [-0.0000,  0.2959,  0.5791,  0.3767,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.2181, -0.0000,  0.0187, -0.6797, -0.9395]]],\n",
       "       dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=lambda x: Mlab(x,x,x,None)\n",
    "mha_out=a(position_encoded_embedding)\n",
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4b5157f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerNormalizationBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim,dtype=torch.float16))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim,dtype=torch.float16))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        standard_deviation=x.std(dim=-1,keepdim=True,unbiased=False)\n",
    "        normalized_x=(x-mean)/(standard_deviation+self.eps)\n",
    "        scale_n_shift=self.scale*normalized_x+self.shift\n",
    "        return scale_n_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ac5fbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnb=layerNormalizationBlock(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3def1824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2305, -1.9590,  0.0428,  0.5586,  1.3242, -0.1959],\n",
       "         [-0.3616, -1.8145,  0.4119,  0.4849,  1.4619, -0.1835],\n",
       "         [-1.8955,  1.3135, -0.0771,  0.8130, -0.0771, -0.0771]],\n",
       "\n",
       "        [[-0.5869,  1.9268, -0.4578,  0.2966, -1.2773,  0.0978],\n",
       "         [-0.9272,  0.3879,  1.6475,  0.7471, -0.9272, -0.9272],\n",
       "         [ 0.5444,  1.0596,  0.5444,  0.5884, -1.0615, -1.6758]]],\n",
       "       dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_normalized_out=lnb(mha_out)\n",
    "layer_normalized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "db6765f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipConnection(nn.Module):\n",
    "    def __init__(self,dropout):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        output=x+sublayer(x)\n",
    "        dropped_output=self.dropout(output)\n",
    "        return dropped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "255723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipConnection=skipConnection(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0bc5de38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, 13.9375, -2.1406, -0.0000,  0.0000, -0.5205],\n",
       "         [-3.7773, -2.5234,  3.9180, -6.6367,  6.6133,  0.1854],\n",
       "         [-1.2666, -1.8066, -3.1191,  0.0000,  4.9180,  0.4897]],\n",
       "\n",
       "        [[-4.2305, -0.1653,  3.3828, -5.2227,  0.0000, -0.0000],\n",
       "         [-0.5723,  0.0000,  0.0000,  0.5381,  0.0000,  0.0000],\n",
       "         [-0.7319, -1.6172, -1.1230,  1.5918,  4.8711, -0.7729]]],\n",
       "       dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_connections_output=skipConnection(position_encoded_embedding,a)\n",
    "skip_connections_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4ca6d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward_block(nn.Module):\n",
    "    ### Expansion Contraction layer.....\n",
    "    def __init__(self,emb_dim,expand_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.expand_dim=expand_dim\n",
    "        self.dropout=dropout\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(emb_dim,expand_dim,dtype=torch.float16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(expand_dim,emb_dim,dtype=torch.float16),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        output=self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e7fdb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffb=feed_forward_block(6,12,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d9b827a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.6777e+00,  1.0498e+00,  1.9746e+00, -1.1768e+00, -9.3408e-01,\n",
       "           1.6172e+00],\n",
       "         [ 4.2773e-01,  2.5195e-01, -2.7832e-01, -3.5864e-01,  3.8452e-01,\n",
       "          -8.3301e-01],\n",
       "         [ 3.5352e-01,  6.7578e-01, -4.8633e-01, -5.7959e-01,  1.0527e+00,\n",
       "           3.7427e-01]],\n",
       "\n",
       "        [[ 1.9946e-01, -1.4912e+00, -1.7651e-01, -9.8926e-01,  8.4863e-01,\n",
       "          -8.2471e-01],\n",
       "         [-2.0898e-01, -3.0420e-01,  3.7964e-02, -9.2316e-04,  4.3854e-02,\n",
       "           1.6431e-01],\n",
       "         [-2.4658e-01, -1.9458e-01, -8.0762e-01, -2.5586e-01,  1.1094e+00,\n",
       "           3.7573e-01]]], dtype=torch.float16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffb_output=ffb(skip_connections_output)\n",
    "ffb_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cb98c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout,mask):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.ff_dropout=ff_dropout\n",
    "        self.expand_dim=expand_dim\n",
    "        self.mha_dropout=mha_dropout\n",
    "        self.sk_dropout=sk_dropout\n",
    "        self.mask=mask\n",
    "        self.layerNormalizationBlocks=[layerNormalizationBlock(self.emb_dim) for _ in range(2)]\n",
    "        self.mha_block=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout)\n",
    "        self.feed_forward_block=feed_forward_block(self.emb_dim,self.expand_dim,self.ff_dropout)\n",
    "        self.skip_connections=[skipConnection(self.sk_dropout) for _ in range(2)]\n",
    "    def forward(self,x):\n",
    "        output1=self.skip_connections[0](x,lambda x: self.mha_block(x,x,x,self.mask))\n",
    "        layer_normalized_output1=self.layerNormalizationBlocks[0](output1)\n",
    "        output2=self.skip_connections[1](output1,self.feed_forward_block)\n",
    "        layer_normalized_output2=self.layerNormalizationBlocks[1](output2)\n",
    "        return layer_normalized_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbda67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
