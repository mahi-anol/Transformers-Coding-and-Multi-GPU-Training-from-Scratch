{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8468ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7ed518e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.emb_dim=emb_dim\n",
    "        self.embedding_layer=nn.Embedding(self.vocab_size,self.emb_dim,dtype=torch.float16)\n",
    "    def forward(self,x):\n",
    "        embeddings=self.embedding_layer(x)\n",
    "        return embeddings*math.sqrt(self.emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f724db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=inputEmbeddingLayer(10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c0416b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.7285, -0.6577,  1.6963, -0.4160, -0.3901,  0.7314],\n",
       "         [ 3.3672,  3.5195, -8.0234, -2.4414, -0.8682, -0.6167],\n",
       "         [-4.2539, -2.8965, -4.1680,  1.3740,  0.8804,  0.2313]],\n",
       "\n",
       "        [[-0.3079, -0.8960, -5.2422, -1.0273, -2.6914,  1.0000],\n",
       "         [-0.6484,  1.1846, -2.8105, -1.2402, -0.4946,  1.7822],\n",
       "         [ 5.4766,  2.6523,  7.1680,  1.9941,  0.6685,  0.2140]]],\n",
       "       dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input=torch.tensor([[1,2,3],[4,5,6]])\n",
    "input_embeddings=embedding_layer(test_input)\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "38abc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncodingLayer(nn.Module):\n",
    "    def __init__(self,max_seq_len,emb_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        static_positional_info=torch.zeros((self.max_seq_len,self.emb_dim),dtype=torch.float16)\n",
    "        positions=torch.arange(0,max_seq_len,dtype=torch.float16).reshape(max_seq_len,-1)\n",
    "        indices_for_denominator=torch.arange(0,emb_dim,2,dtype=torch.float16) ### 2i\n",
    "        denominators=torch.exp((-2*indices_for_denominator*math.log(1e4))/emb_dim)\n",
    "        static_positional_info[:,0::2]=torch.sin(positions*denominators)\n",
    "        static_positional_info[:,1::2]=torch.cos(positions*denominators)\n",
    "        self.register_buffer('static_positional_info',static_positional_info)\n",
    "    def forward(self,x):\n",
    "        position_encoded_embedding=x+self.static_positional_info[:x.shape[1],:]\n",
    "        dropped_embeddings=self.dropout(position_encoded_embedding)\n",
    "        return dropped_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3326665",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding_layer=positionalEncodingLayer(3,6,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "83f95915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -5.3281,   0.4890,   0.0000,   0.8345,  -0.0000,   0.0000],\n",
       "         [  6.0117,   5.7969, -11.4609,  -0.0000,  -0.0000,   0.0000],\n",
       "         [ -0.0000,  -0.0000,  -5.9492,   0.0000,   0.0000,   1.7598]],\n",
       "\n",
       "        [[ -0.0000,   0.0000,  -7.4883,  -0.0391,  -3.8457,   2.8574],\n",
       "         [  0.0000,   2.4648,  -0.0000,  -0.0000,  -0.7065,   3.9727],\n",
       "         [  9.1250,   3.1953,   0.0000,   4.2773,   0.0000,   1.7344]]],\n",
       "       dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoded_embedding=positional_encoding_layer(input_embeddings)\n",
    "position_encoded_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01dc91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,dropout):\n",
    "        super().__init__()\n",
    "        assert emb_dim%n_heads==0 ### checking if multi head splitting is possible.\n",
    "        self.w_q=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_k=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_v=nn.Linear(emb_dim,emb_dim,dtype=torch.float16)\n",
    "        self.w_o=nn.Linear(emb_dim,emb_dim,dtype=torch.float16) ### multi-head-projection-layer\n",
    "        self.emb_dim=emb_dim\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.single_head_dim=self.emb_dim//n_heads\n",
    "        self.n_heads=n_heads\n",
    "\n",
    "    @staticmethod\n",
    "    def contextual_embedding(m_q,m_k,m_v,per_head_emb_dim,mask):\n",
    "        ### return contexual embedding and attention scores\n",
    "        attention_scores=m_q@m_k.transpose(2,3)/math.sqrt(per_head_emb_dim)\n",
    "        ##batch,head,seq,dim @ batch,head,dim,seq==batch,head,seq,seq\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask,value=float('-inf'))\n",
    "        normalized_attention_scores=torch.softmax(attention_scores,dim=-1)\n",
    "        ### batch,head,seq,seq @ batch,head,seq,dim=batch,head,seq,dim\n",
    "        contexual_embeddings=normalized_attention_scores@m_v\n",
    "        return normalized_attention_scores,contexual_embeddings\n",
    "    \n",
    "    def forward(self,q,k,v,mask):\n",
    "        query=self.w_q(q) ### batch, seqeunce, dim\n",
    "        key=self.w_k(k)\n",
    "        value=self.w_v(v)\n",
    "\n",
    "        multihead_query=query.view(query.shape[0],query.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        multihead_key=key.view(key.shape[0],key.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        multihead_value=value.view(value.shape[0],value.shape[1],self.n_heads,self.single_head_dim).transpose(1,2)\n",
    "        _,contextual_embeddings=multiHeadAttentionBlock.contextual_embedding(multihead_query,multihead_key,multihead_value,self.single_head_dim,mask)\n",
    "        final_contextual_embeddings=contextual_embeddings.transpose(1,2).contiguous().view(value.shape[0],value.shape[1],self.n_heads*self.single_head_dim)\n",
    "        multihead_final_contextual_embedding_proj=self.w_o(final_contextual_embeddings)\n",
    "        dropped_multihead_final_contextual_embedding_proj=self.dropout(multihead_final_contextual_embedding_proj)\n",
    "        return dropped_multihead_final_contextual_embedding_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "399b28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mlab= multiHeadAttentionBlock(6,2,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ecd1facc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4980e+00, -1.6797e-01,  7.0264e-01,  3.2734e+00, -3.0156e+00,\n",
       "           1.5723e+00],\n",
       "         [ 3.6279e-01,  6.8555e-01,  1.4868e-01,  0.0000e+00, -1.5420e+00,\n",
       "           1.8887e+00],\n",
       "         [ 2.2095e-01,  7.1387e-01, -1.4087e-01,  6.7383e-02, -9.2285e-01,\n",
       "           1.3086e+00]],\n",
       "\n",
       "        [[ 5.4395e-01,  4.7998e-01,  1.5625e+00,  0.0000e+00, -0.0000e+00,\n",
       "           2.2129e+00],\n",
       "         [-1.4048e-03,  4.1260e-01,  1.1982e+00,  1.6797e+00, -3.3379e+00,\n",
       "           2.0293e+00],\n",
       "         [ 7.5391e-01,  3.4570e-01,  1.6953e+00,  2.3438e-01, -2.6816e+00,\n",
       "           2.1367e+00]]], dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=lambda x: Mlab(x,x,x,None)\n",
    "mha_out=a(position_encoded_embedding)\n",
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4b5157f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerNormalizationBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim,dtype=torch.float16))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim,dtype=torch.float16))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        standard_deviation=x.std(dim=-1,keepdim=True,unbiased=False)\n",
    "        normalized_x=(x-mean)/(standard_deviation+self.eps)\n",
    "        scale_n_shift=self.scale*normalized_x+self.shift\n",
    "        return scale_n_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac5fbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnb=layerNormalizationBlock(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3def1824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1260, -0.0663,  0.3296,  1.4980, -1.3613,  0.7251],\n",
       "         [ 0.1039,  0.4221, -0.1071, -0.2537, -1.7725,  1.6074],\n",
       "         [ 0.0188,  0.7290, -0.5024, -0.2025, -1.6299,  1.5859]],\n",
       "\n",
       "        [[-0.3123, -0.3904,  0.9307, -0.9761, -0.9761,  1.7246],\n",
       "         [-0.1862,  0.0463,  0.4873,  0.7578, -2.0586,  0.9541],\n",
       "         [ 0.2198, -0.0442,  0.8286, -0.1162, -2.0020,  1.1143]]],\n",
       "       dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_normalized_out=lnb(mha_out)\n",
    "layer_normalized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db6765f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipConnection(nn.Module):\n",
    "    def __init__(self,dropout):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        output=x+sublayer(x)\n",
    "        dropped_output=self.dropout(output)\n",
    "        return dropped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "255723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipConnectionLayer=skipConnection(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bc5de38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000,  0.4587,  0.0000,  5.8711, -4.3086,  0.0000],\n",
       "         [ 9.1094,  9.2656, -0.0000,  0.0000, -0.0000,  2.6992],\n",
       "         [ 0.3157,  1.0195, -8.7031,  0.0963, -0.0000,  4.3828]],\n",
       "\n",
       "        [[ 0.0000,  0.0000, -0.0000, -0.0558, -0.0000,  7.2422],\n",
       "         [ 0.0000,  3.5215,  0.0000,  0.0000, -5.7773,  8.5703],\n",
       "         [ 0.0000,  4.5664,  0.0000,  6.4453, -0.0000,  5.5312]]],\n",
       "       dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_connections_output=skipConnectionLayer(position_encoded_embedding,a)\n",
    "skip_connections_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4ca6d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward_block(nn.Module):\n",
    "    ### Expansion Contraction layer.....\n",
    "    def __init__(self,emb_dim,expand_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.expand_dim=expand_dim\n",
    "        self.dropout=dropout\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(emb_dim,expand_dim,dtype=torch.float16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(expand_dim,emb_dim,dtype=torch.float16),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        output=self.network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e7fdb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffb=feed_forward_block(6,12,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d9b827a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1298,  0.8462,  1.4238, -0.2001, -1.5342, -2.2734],\n",
       "         [-0.6792, -1.0068,  1.0400,  0.0239, -1.4814, -1.7031],\n",
       "         [ 0.6167,  0.8081, -0.4392, -0.6909,  0.4221,  0.7646]],\n",
       "\n",
       "        [[-0.1832,  0.4194,  0.3806, -0.4565,  0.8589,  0.1782],\n",
       "         [ 0.3315,  1.0537, -0.2401, -0.8760,  0.7412,  0.7852],\n",
       "         [-0.5298, -0.3499,  0.5244,  0.3955, -1.9307, -1.4697]]],\n",
       "       dtype=torch.float16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffb_output=ffb(skip_connections_output)\n",
    "ffb_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cb98c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.ff_dropout=ff_dropout\n",
    "        self.expand_dim=expand_dim\n",
    "        self.mha_dropout=mha_dropout\n",
    "        self.sk_dropout=sk_dropout\n",
    "        self.mha_block=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout)\n",
    "        self.feed_forward_block=feed_forward_block(self.emb_dim,self.expand_dim,self.ff_dropout)\n",
    "        self.skip_connections=nn.ModuleList([skipConnection(self.sk_dropout) for _ in range(2)])\n",
    "        self.layerNormalizationBlocks=nn.ModuleList([layerNormalizationBlock(self.emb_dim) for _ in range(2)])\n",
    "    def forward(self,x,mask=None):\n",
    "        output1=self.skip_connections[0](x,lambda x: self.mha_block(x,x,x,mask))\n",
    "        layer_normalized_output1=self.layerNormalizationBlocks[0](output1)\n",
    "        output2=self.skip_connections[1](layer_normalized_output1,self.feed_forward_block)\n",
    "        layer_normalized_output2=self.layerNormalizationBlocks[1](output2)\n",
    "        return layer_normalized_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ebbda67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoderBlock(\n",
       "  (mha_block): multiHeadAttentionBlock(\n",
       "    (w_q): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_k): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_v): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_o): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (feed_forward_block): feed_forward_block(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=6, out_features=12, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=12, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (skip_connections): ModuleList(\n",
       "    (0-1): 2 x skipConnection(\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layerNormalizationBlocks): ModuleList(\n",
       "    (0-1): 2 x layerNormalizationBlock()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_blk=encoderBlock(6,2,0.3,12,0.3,0.3)\n",
    "enc_blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3ec1a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3645,  0.2683,  1.3809,  0.5864, -1.5908, -1.0088],\n",
       "         [-0.1202, -0.6440,  0.4849,  1.6025,  0.3086, -1.6338],\n",
       "         [ 0.7104,  0.3320, -1.2617, -1.4355,  0.3979,  1.2559]],\n",
       "\n",
       "        [[-0.9526,  0.1586,  0.7988, -1.2871, -0.3452,  1.6279],\n",
       "         [ 0.1030,  0.1030, -0.6357, -1.4971,  0.1030,  1.8242],\n",
       "         [ 0.0309, -1.2832,  0.5620,  1.7129, -1.0537,  0.0309]]],\n",
       "       dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_blk(ffb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "64360101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,no_of_enc_blk,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.enc_blks=nn.ModuleList([encoderBlock(emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout) for _ in range(no_of_enc_blk)])\n",
    "    def forward(self,x,mask=None):\n",
    "        for blk in self.enc_blks:\n",
    "            x=blk(x,mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0faa8ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (enc_blks): ModuleList(\n",
       "    (0-11): 12 x encoderBlock(\n",
       "      (mha_block): multiHeadAttentionBlock(\n",
       "        (w_q): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_k): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_v): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_o): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (feed_forward_block): feed_forward_block(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=6, out_features=12, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=12, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (skip_connections): ModuleList(\n",
       "        (0-1): 2 x skipConnection(\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layerNormalizationBlocks): ModuleList(\n",
       "        (0-1): 2 x layerNormalizationBlock()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc=encoder(12,6,2,0.3,12,0.3,0.3)\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4ab7cc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3179, -1.1045, -0.2343, -0.2343,  2.1250, -0.2343],\n",
       "         [ 0.0470, -0.1120,  0.0470, -1.9609,  0.6914,  1.2881],\n",
       "         [-1.9170,  0.2368,  0.4348, -0.0729,  1.4346, -0.1168]],\n",
       "\n",
       "        [[-0.2869,  1.9961,  0.0468, -0.3589, -0.0558, -1.3408],\n",
       "         [-0.9761, -0.6025,  0.8018,  1.7236,  0.0816, -1.0303],\n",
       "         [ 1.6953, -1.1719, -0.7769, -0.7769,  0.4128,  0.6157]]],\n",
       "       dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output=enc(ffb_output)\n",
    "enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6a728f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.ff_dropout=ff_dropout\n",
    "        self.expand_dim=expand_dim\n",
    "        self.mha_dropout=mha_dropout\n",
    "        self.sk_dropout=sk_dropout\n",
    "        self.mha_block1=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout) ### casual attention block\n",
    "        self.mha_block2=multiHeadAttentionBlock(self.emb_dim,self.n_heads,self.mha_dropout) #### cross attention block\n",
    "        self.feed_forward_block=feed_forward_block(self.emb_dim,self.expand_dim,self.ff_dropout)\n",
    "        self.skip_connections=nn.ModuleList([skipConnection(self.sk_dropout) for _ in range(3)])\n",
    "        self.layerNormalizationBlocks=nn.ModuleList([layerNormalizationBlock(self.emb_dim) for _ in range(3)])\n",
    "    def forward(self,x,enc_out,mask1=None,mask2=None):\n",
    "        output1=self.skip_connections[0](x,lambda x: self.mha_block1(x,x,x,mask1))\n",
    "        layer_normalized_output1=self.layerNormalizationBlocks[0](output1)\n",
    "        output2=self.skip_connections[1](layer_normalized_output1,lambda x: self.mha_block2(x,enc_out,enc_out,mask2))\n",
    "        layer_normalized_output2=self.layerNormalizationBlocks[1](output2)\n",
    "        output3=self.skip_connections[2](layer_normalized_output2,self.feed_forward_block)\n",
    "        layer_normalized_output3=self.layerNormalizationBlocks[2](output3)\n",
    "        return layer_normalized_output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "221702c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoderBlock(\n",
       "  (mha_block1): multiHeadAttentionBlock(\n",
       "    (w_q): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_k): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_v): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_o): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (mha_block2): multiHeadAttentionBlock(\n",
       "    (w_q): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_k): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_v): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (w_o): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (feed_forward_block): feed_forward_block(\n",
       "    (network): Sequential(\n",
       "      (0): Linear(in_features=6, out_features=12, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=12, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (skip_connections): ModuleList(\n",
       "    (0-2): 3 x skipConnection(\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layerNormalizationBlocks): ModuleList(\n",
       "    (0-2): 3 x layerNormalizationBlock()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_blk=decoderBlock(6,2,0.3,12,0.3,0.3)\n",
    "dec_blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "433e0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7739,  1.6338,  0.2025,  0.3301,  0.2025, -1.5947],\n",
       "         [-0.8677,  0.5093,  0.3181,  0.3696,  1.3672, -1.6973],\n",
       "         [ 0.0753,  1.7529, -0.3643, -1.5625,  0.4619, -0.3633]],\n",
       "\n",
       "        [[-0.2925,  0.1620, -1.7334,  0.1620,  0.0135,  1.6895],\n",
       "         [-0.8613,  0.8999,  0.0722, -1.5146, -0.0594,  1.4648],\n",
       "         [-0.4255, -0.5322, -0.4255, -0.4255,  2.2344, -0.4255]]],\n",
       "       dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_blk_out=dec_blk(ffb_output,ffb_output)\n",
    "dec_blk_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c11614b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self,no_of_enc_blk,emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout):\n",
    "        super().__init__()\n",
    "        self.dec_blks=nn.ModuleList([decoderBlock(emb_dim,n_heads,mha_dropout,expand_dim,ff_dropout,sk_dropout) for _ in range(no_of_enc_blk)])\n",
    "    def forward(self,x,mask=None):\n",
    "        for blk in self.dec_blks:\n",
    "            x=blk(x,mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5b549c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder(\n",
       "  (dec_blks): ModuleList(\n",
       "    (0-11): 12 x decoderBlock(\n",
       "      (mha_block1): multiHeadAttentionBlock(\n",
       "        (w_q): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_k): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_v): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_o): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (mha_block2): multiHeadAttentionBlock(\n",
       "        (w_q): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_k): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_v): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (w_o): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (feed_forward_block): feed_forward_block(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=6, out_features=12, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=12, out_features=6, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (skip_connections): ModuleList(\n",
       "        (0-2): 3 x skipConnection(\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layerNormalizationBlocks): ModuleList(\n",
       "        (0-2): 3 x layerNormalizationBlock()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec=decoder(12,6,2,0.3,12,0.3,0.3)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1b3427a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6748, -0.4148,  0.6748,  0.4541,  0.6748, -2.0625],\n",
       "         [ 0.6167,  0.9536,  0.7568, -1.9814,  0.1019, -0.4482],\n",
       "         [ 1.6670,  0.7524, -0.4072, -1.5361, -0.3074, -0.1692]],\n",
       "\n",
       "        [[ 0.3938,  0.9111, -1.3838,  1.0459,  0.3938, -1.3623],\n",
       "         [ 0.9048,  0.3977, -1.8027,  0.4558, -0.8594,  0.9048],\n",
       "         [ 0.2045, -0.8579, -1.1289,  0.6162,  1.7852, -0.6191]]],\n",
       "       dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output=dec(ffb_output,ffb_output)\n",
    "dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "480ad90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class finalProjectionLayer(nn.Module):\n",
    "    def __init__(self,emb_dim,vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(emb_dim,vocab_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output=self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformers(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.encoder=\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
